{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae28ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Processing ne.txt in chunks...\n",
      "  Processed 10 chunks (10,485,760 chars)...\n",
      "  Processed 20 chunks (20,971,520 chars)...\n",
      "  Processed 30 chunks (31,457,280 chars)...\n",
      "  Processed 40 chunks (41,943,040 chars)...\n",
      "  Processed 50 chunks (52,428,800 chars)...\n",
      "  Processed 60 chunks (62,914,560 chars)...\n",
      "  Processed 70 chunks (73,400,320 chars)...\n",
      "  Processed 80 chunks (83,886,080 chars)...\n",
      "  Processed 90 chunks (94,371,840 chars)...\n",
      "  Processed 100 chunks (104,857,600 chars)...\n",
      "  Processed 110 chunks (115,343,360 chars)...\n",
      "  Processed 120 chunks (125,829,120 chars)...\n",
      "  Processed 130 chunks (136,314,880 chars)...\n",
      "  Processed 140 chunks (146,800,640 chars)...\n",
      "  Processed 150 chunks (157,286,400 chars)...\n",
      "  Processed 160 chunks (167,772,160 chars)...\n",
      "  Processed 170 chunks (178,257,920 chars)...\n",
      "  Processed 180 chunks (188,743,680 chars)...\n",
      "  Processed 190 chunks (199,229,440 chars)...\n",
      "  Processed 200 chunks (209,715,200 chars)...\n",
      "  Processed 210 chunks (220,200,960 chars)...\n",
      "  Processed 220 chunks (230,686,720 chars)...\n",
      "  Processed 230 chunks (241,172,480 chars)...\n",
      "  Processed 240 chunks (251,658,240 chars)...\n",
      "  Processed 250 chunks (262,144,000 chars)...\n",
      "  Processed 260 chunks (272,629,760 chars)...\n",
      "  Processed 270 chunks (283,115,520 chars)...\n",
      "  Processed 280 chunks (293,601,280 chars)...\n",
      "  Processed 290 chunks (304,087,040 chars)...\n",
      "  Processed 300 chunks (314,572,800 chars)...\n",
      "  Processed 310 chunks (325,058,560 chars)...\n",
      "  Processed 320 chunks (335,544,320 chars)...\n",
      "  Processed 330 chunks (346,030,080 chars)...\n",
      "  Processed 340 chunks (356,515,840 chars)...\n",
      "  Processed 350 chunks (367,001,600 chars)...\n",
      "  Processed 360 chunks (377,487,360 chars)...\n",
      "  Processed 370 chunks (387,973,120 chars)...\n",
      "  Processed 380 chunks (398,458,880 chars)...\n",
      "  Processed 390 chunks (408,944,640 chars)...\n",
      "  Processed 400 chunks (419,430,400 chars)...\n",
      "  Processed 410 chunks (429,916,160 chars)...\n",
      "  Processed 420 chunks (440,401,920 chars)...\n",
      "  Processed 430 chunks (450,887,680 chars)...\n",
      "  Processed 440 chunks (461,373,440 chars)...\n",
      "  Processed 450 chunks (471,859,200 chars)...\n",
      "  Processed 460 chunks (482,344,960 chars)...\n",
      "  Processed 470 chunks (492,830,720 chars)...\n",
      "  Processed 480 chunks (503,316,480 chars)...\n",
      "  Processed 490 chunks (513,802,240 chars)...\n",
      "  Processed 500 chunks (524,288,000 chars)...\n",
      "  Processed 510 chunks (534,773,760 chars)...\n",
      "  Processed 520 chunks (545,259,520 chars)...\n",
      "  Processed 530 chunks (555,745,280 chars)...\n",
      "  Processed 540 chunks (566,231,040 chars)...\n",
      "  Processed 550 chunks (576,716,800 chars)...\n",
      "  Processed 560 chunks (587,202,560 chars)...\n",
      "  Processed 570 chunks (597,688,320 chars)...\n",
      "  Processed 580 chunks (608,174,080 chars)...\n",
      "  Processed 590 chunks (618,659,840 chars)...\n",
      "  Processed 600 chunks (629,145,600 chars)...\n",
      "  Processed 610 chunks (639,631,360 chars)...\n",
      "  Processed 620 chunks (650,117,120 chars)...\n",
      "  Processed 630 chunks (660,602,880 chars)...\n",
      "  Processed 640 chunks (671,088,640 chars)...\n",
      "  Processed 650 chunks (681,574,400 chars)...\n",
      "  Processed 660 chunks (692,060,160 chars)...\n",
      "  Processed 670 chunks (702,545,920 chars)...\n",
      "  Processed 680 chunks (713,031,680 chars)...\n",
      "  Processed 690 chunks (723,517,440 chars)...\n",
      "  Processed 700 chunks (734,003,200 chars)...\n",
      "  Processed 710 chunks (744,488,960 chars)...\n",
      "  Processed 720 chunks (754,974,720 chars)...\n",
      "  Processed 730 chunks (765,460,480 chars)...\n",
      "  Processed 740 chunks (775,946,240 chars)...\n",
      "  Processed 750 chunks (786,432,000 chars)...\n",
      "  Processed 760 chunks (796,917,760 chars)...\n",
      "  Processed 770 chunks (807,403,520 chars)...\n",
      "  Processed 780 chunks (817,889,280 chars)...\n",
      "  Processed 790 chunks (828,375,040 chars)...\n",
      "  Processed 800 chunks (838,860,800 chars)...\n",
      "  Processed 810 chunks (849,346,560 chars)...\n",
      "  Processed 820 chunks (859,832,320 chars)...\n",
      "  Processed 830 chunks (870,318,080 chars)...\n",
      "  Processed 840 chunks (880,803,840 chars)...\n",
      "  Processed 850 chunks (891,289,600 chars)...\n",
      "  Processed 860 chunks (901,775,360 chars)...\n",
      "  Processed 870 chunks (912,261,120 chars)...\n",
      "  Processed 880 chunks (922,746,880 chars)...\n",
      "  Processed 890 chunks (933,232,640 chars)...\n",
      "  Processed 900 chunks (943,718,400 chars)...\n",
      "  Processed 910 chunks (954,204,160 chars)...\n",
      "  Processed 920 chunks (964,689,920 chars)...\n",
      "  Processed 930 chunks (975,175,680 chars)...\n",
      "  Processed 940 chunks (985,661,440 chars)...\n",
      "  Processed 950 chunks (996,147,200 chars)...\n",
      "  Processed 960 chunks (1,006,632,960 chars)...\n",
      "  Processed 970 chunks (1,017,118,720 chars)...\n",
      "  Processed 980 chunks (1,027,604,480 chars)...\n",
      "  Processed 990 chunks (1,038,090,240 chars)...\n",
      "  Processed 1000 chunks (1,048,576,000 chars)...\n",
      "  Processed 1010 chunks (1,059,061,760 chars)...\n",
      "  Processed 1020 chunks (1,069,547,520 chars)...\n",
      "  Processed 1030 chunks (1,080,033,280 chars)...\n",
      "  Processed 1040 chunks (1,090,519,040 chars)...\n",
      "  Processed 1050 chunks (1,101,004,800 chars)...\n",
      "  Processed 1060 chunks (1,111,490,560 chars)...\n",
      "  Processed 1070 chunks (1,121,976,320 chars)...\n",
      "  Processed 1080 chunks (1,132,462,080 chars)...\n",
      "  Processed 1090 chunks (1,142,947,840 chars)...\n",
      "  Processed 1100 chunks (1,153,433,600 chars)...\n",
      "  Processed 1110 chunks (1,163,919,360 chars)...\n",
      "  Processed 1120 chunks (1,174,405,120 chars)...\n",
      "  Processed 1130 chunks (1,184,890,880 chars)...\n",
      "  Processed 1140 chunks (1,195,376,640 chars)...\n",
      "  Processed 1150 chunks (1,205,862,400 chars)...\n",
      "  Processed 1160 chunks (1,216,348,160 chars)...\n",
      "  Processed 1170 chunks (1,226,833,920 chars)...\n",
      "  Processed 1180 chunks (1,237,319,680 chars)...\n",
      "  Processed 1190 chunks (1,247,805,440 chars)...\n",
      "  Processed 1200 chunks (1,258,291,200 chars)...\n",
      "  Processed 1210 chunks (1,268,776,960 chars)...\n",
      "  Processed 1220 chunks (1,279,262,720 chars)...\n",
      "  Processed 1230 chunks (1,289,748,480 chars)...\n",
      "  Processed 1240 chunks (1,300,234,240 chars)...\n",
      "  Processed 1250 chunks (1,310,720,000 chars)...\n",
      "  Processed 1260 chunks (1,321,205,760 chars)...\n",
      "  Processed 1270 chunks (1,331,691,520 chars)...\n",
      "  Processed 1280 chunks (1,342,177,280 chars)...\n",
      "  Processed 1290 chunks (1,352,663,040 chars)...\n",
      "  Processed 1300 chunks (1,363,148,800 chars)...\n",
      "  Processed 1310 chunks (1,373,634,560 chars)...\n",
      "  Processed 1320 chunks (1,384,120,320 chars)...\n",
      "  Processed 1330 chunks (1,394,606,080 chars)...\n",
      "  Processed 1340 chunks (1,405,091,840 chars)...\n",
      "  Processed 1350 chunks (1,415,577,600 chars)...\n",
      "  Processed 1360 chunks (1,426,063,360 chars)...\n",
      "  Processed 1370 chunks (1,436,549,120 chars)...\n",
      "  Processed 1380 chunks (1,447,034,880 chars)...\n",
      "  Processed 1390 chunks (1,457,520,640 chars)...\n",
      "  Processed 1400 chunks (1,468,006,400 chars)...\n",
      "  Processed 1410 chunks (1,478,492,160 chars)...\n",
      "  Processed 1420 chunks (1,488,977,920 chars)...\n",
      "  Processed 1430 chunks (1,499,463,680 chars)...\n",
      "  Processed 1440 chunks (1,509,949,440 chars)...\n",
      "  Processed 1450 chunks (1,520,435,200 chars)...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 1,528,015,513 chars\n",
      "Cleaned: 1,521,583,389 chars\n",
      "Removed: 6,432,124 chars\n",
      "\n",
      "--- Sample of cleaned text ---\n",
      "‡§µ‡•à‡§∂‡§æ‡§ñ ‡•®‡•ß ‡§Ü‡§∞‡•ç‡§∏‡§®‡§≤‡§≤‡§æ‡§à ‡§π‡§∞‡§æ‡§â‡§Å‡§¶‡•à ‡§è‡§•‡•ç‡§≤‡•á‡§ü‡§ø‡§ï‡•ã ‡§Æ‡§°‡•ç‡§∞‡§ø‡§° ‡§Ø‡•Å‡§∞‡•ã‡§™‡§æ ‡§≤‡§ø‡§ó‡§ï‡•ã ‡§´‡§æ‡§á‡§®‡§≤‡§Æ‡§æ ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§ó‡§∞‡•á‡§ï‡•ã ‡§õ ‡•§\n",
      "‡§ò‡§∞‡•á‡§≤‡•Å ‡§Æ‡•à‡§¶‡§æ‡§®‡§Æ‡§æ ‡§≠‡§è‡§ï‡•ã ‡§ö‡•ç‡§Ø‡§æ‡§Æ‡•ç‡§™‡§ø‡§Ø‡§®‡•ç‡§∏ ‡§≤‡§ø‡§ó‡§ï‡•ã ‡§¶‡•ã‡§∏‡•ç‡§∞‡•ã ‡§≤‡•á‡§ó‡§Æ‡§æ ‡§è‡§•‡•ç‡§≤‡•á‡§ü‡§ø‡§ï‡•ã ‡§Æ‡§°‡•ç‡§∞‡§ø‡§°‡§≤‡•á ‡§Ü‡§∞‡•ç‡§∏‡§®‡§≤‡§≤‡§æ‡§à ‡§è‡§ï ‡§∂‡•Ç‡§®‡•ç‡§Ø‡§≤‡•á ‡§π‡§∞‡§æ‡§â‡§Å‡§¶‡•à ‡§∏‡§Æ‡§ó‡•ç‡§∞‡§Æ‡§æ ‡§¶‡•Å‡§à ‡§è‡§ï‡§ï‡•ã ‡§Ö‡§ó‡•ç‡§∞‡§§‡§æ‡§ï‡§æ ‡§∏‡§æ‡§• ‡§´‡§æ‡§á‡§®‡§≤‡§Æ‡§æ ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§ó‡§∞‡•á‡§ï‡•ã ‡§π‡•ã ‡•§\n",
      "‡§∏‡§®‡•ç ‡•®‡•¶‡•ß‡•¨ ‡§ï‡•ã ‡§ö‡•ç‡§Ø‡§æ‡§Æ‡•ç‡§™‡§ø‡§Ø‡§®‡•ç‡§∏ ‡§≤‡§ø‡§ó‡§ï‡•ã ‡§´‡§æ‡§á‡§®‡§≤‡§Æ‡§æ ‡§∞‡§ø‡§Ø‡§≤ ‡§Æ‡§°‡•ç‡§∞‡§ø‡§°‡§∏‡§Å‡§ó ‡§™‡§∞‡§æ‡§ú‡§ø‡§§ ‡§≠‡§è‡§™‡§õ‡§ø ‡§è‡§•‡•ç‡§≤‡•á‡§ü‡§ø‡§ï‡•ã ‡§™‡§π‡§ø‡§≤‡•ã ‡§™‡§ü‡§ï ‡§Ø‡•Å‡§∞‡•ã‡§™‡§ø‡§Ø‡§® ‡§´‡•Å‡§ü‡§¨‡§≤ ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ‡§ï‡•ã ‡§´‡§æ‡§á‡§®‡§≤‡§Æ‡§æ ‡§™‡•Å‡§ó‡•á‡§ï‡•ã ‡§π‡•ã ‡•§\n",
      "‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§µ‡§ø‡§ô‡§ó‡§∞‡§≤‡•á ‡§Ø‡•Å‡§∞‡•ã‡§™‡§ø‡§Ø‡§® ‡§ñ‡•á‡§≤‡§Æ‡§æ ‡§∏‡§´‡§≤‡§§‡§æ ‡§π‡§æ‡§§ ‡§™‡§æ‡§∞‡•ç‡§® ‡§∏‡§ï‡•á‡§®‡§®‡•ç ‡•§ ‡§™‡•ç‡§∞‡§ø‡§Æ‡§ø‡§Ø‡§∞ ‡§≤‡§ø‡§ó‡§Æ‡§æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§®‡§∏‡§ï‡•á‡§™‡§õ‡§ø ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï‡§ï‡§æ ‡§∞‡•Å‡§™‡§Æ‡§æ ‡§â‡§®‡§ï‡•ã\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text):\n",
    "    \"\"\"Clean Nepali text while preserving Devanagari punctuation.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Keep Nepali-specific characters\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English\n",
    "        r'\\u0900-\\u097F'             # Devanagari including ‡•§‡••\n",
    "        r'0-9\\u0966-\\u096F'          # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\[\\]{}\\-\\'\\\"/\\\\'   # Punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process file in chunks to avoid memory crashes\n",
    "print(\"üìñ Processing ne.txt in chunks...\")\n",
    "\n",
    "chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "buffer = []\n",
    "total_original = 0\n",
    "total_cleaned = 0\n",
    "\n",
    "try:\n",
    "    with open('ne.txt', 'r', encoding='utf-8') as infile, \\\n",
    "         open('ne_cleaned.txt', 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = infile.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_num += 1\n",
    "            total_original += len(chunk)\n",
    "            \n",
    "            # Clean chunk\n",
    "            cleaned = clean_nepali_text(chunk)\n",
    "            \n",
    "            # Filter lines (minimum 5 chars)\n",
    "            lines = [line.strip() for line in cleaned.split('\\n') \n",
    "                     if len(line.strip()) >= 5]\n",
    "            \n",
    "            # Write to output\n",
    "            if lines:\n",
    "                output = '\\n'.join(lines) + '\\n'\n",
    "                outfile.write(output)\n",
    "                total_cleaned += len(output)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_original:,} chars)...\")\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample of cleaned text ---\")\n",
    "    with open('ne_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read(500))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: ne.txt not found in current directory\")\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Still running out of memory. Try increasing chunk_size or processing smaller sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bc081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ne_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11607422it [00:38, 305216.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ne_corpus_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2508714it [00:11, 217116.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 14,116,136\n",
      "Unique lines written: 6,554,129\n",
      "Output saved to: combined_final.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'ne_cleaned.txt'\n",
    "file2 = 'ne_corpus_cleaned.txt'\n",
    "output_file = 'combined_final.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdebf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Processing ne_corpus.txt in chunks...\n",
      "  Processed 10 chunks (10,485,760 chars)...\n",
      "  Processed 20 chunks (20,971,520 chars)...\n",
      "  Processed 30 chunks (31,457,280 chars)...\n",
      "  Processed 40 chunks (41,943,040 chars)...\n",
      "  Processed 50 chunks (52,428,800 chars)...\n",
      "  Processed 60 chunks (62,914,560 chars)...\n",
      "  Processed 70 chunks (73,400,320 chars)...\n",
      "  Processed 80 chunks (83,886,080 chars)...\n",
      "  Processed 90 chunks (94,371,840 chars)...\n",
      "  Processed 100 chunks (104,857,600 chars)...\n",
      "  Processed 110 chunks (115,343,360 chars)...\n",
      "  Processed 120 chunks (125,829,120 chars)...\n",
      "  Processed 130 chunks (136,314,880 chars)...\n",
      "  Processed 140 chunks (146,800,640 chars)...\n",
      "  Processed 150 chunks (157,286,400 chars)...\n",
      "  Processed 160 chunks (167,772,160 chars)...\n",
      "  Processed 170 chunks (178,257,920 chars)...\n",
      "  Processed 180 chunks (188,743,680 chars)...\n",
      "  Processed 190 chunks (199,229,440 chars)...\n",
      "  Processed 200 chunks (209,715,200 chars)...\n",
      "  Processed 210 chunks (220,200,960 chars)...\n",
      "  Processed 220 chunks (230,686,720 chars)...\n",
      "  Processed 230 chunks (241,172,480 chars)...\n",
      "  Processed 240 chunks (251,658,240 chars)...\n",
      "  Processed 250 chunks (262,144,000 chars)...\n",
      "  Processed 260 chunks (272,629,760 chars)...\n",
      "  Processed 270 chunks (283,115,520 chars)...\n",
      "  Processed 280 chunks (293,601,280 chars)...\n",
      "  Processed 290 chunks (304,087,040 chars)...\n",
      "  Processed 300 chunks (314,572,800 chars)...\n",
      "  Processed 310 chunks (325,058,560 chars)...\n",
      "  Processed 320 chunks (335,544,320 chars)...\n",
      "  Processed 330 chunks (346,030,080 chars)...\n",
      "  Processed 340 chunks (356,515,840 chars)...\n",
      "  Processed 350 chunks (367,001,600 chars)...\n",
      "  Processed 360 chunks (377,487,360 chars)...\n",
      "  Processed 370 chunks (387,973,120 chars)...\n",
      "  Processed 380 chunks (398,458,880 chars)...\n",
      "  Processed 390 chunks (408,944,640 chars)...\n",
      "  Processed 400 chunks (419,430,400 chars)...\n",
      "  Processed 410 chunks (429,916,160 chars)...\n",
      "  Processed 420 chunks (440,401,920 chars)...\n",
      "  Processed 430 chunks (450,887,680 chars)...\n",
      "  Processed 440 chunks (461,373,440 chars)...\n",
      "  Processed 450 chunks (471,859,200 chars)...\n",
      "  Processed 460 chunks (482,344,960 chars)...\n",
      "  Processed 470 chunks (492,830,720 chars)...\n",
      "  Processed 480 chunks (503,316,480 chars)...\n",
      "  Processed 490 chunks (513,802,240 chars)...\n",
      "  Processed 500 chunks (524,288,000 chars)...\n",
      "  Processed 510 chunks (534,773,760 chars)...\n",
      "  Processed 520 chunks (545,259,520 chars)...\n",
      "  Processed 530 chunks (555,745,280 chars)...\n",
      "  Processed 540 chunks (566,231,040 chars)...\n",
      "  Processed 550 chunks (576,716,800 chars)...\n",
      "  Processed 560 chunks (587,202,560 chars)...\n",
      "  Processed 570 chunks (597,688,320 chars)...\n",
      "  Processed 580 chunks (608,174,080 chars)...\n",
      "  Processed 590 chunks (618,659,840 chars)...\n",
      "  Processed 600 chunks (629,145,600 chars)...\n",
      "  Processed 610 chunks (639,631,360 chars)...\n",
      "  Processed 620 chunks (650,117,120 chars)...\n",
      "  Processed 630 chunks (660,602,880 chars)...\n",
      "  Processed 640 chunks (671,088,640 chars)...\n",
      "  Processed 650 chunks (681,574,400 chars)...\n",
      "  Processed 660 chunks (692,060,160 chars)...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 700,102,460 chars\n",
      "Cleaned: 698,005,692 chars\n",
      "Removed: 2,096,768 chars\n",
      "\n",
      "--- Sample of cleaned text ---\n",
      "‡§¨‡§∞‡•ç‡§¶‡§ø‡§¨‡§æ‡§∏ ‡§®‡§ó‡§∞‡§™‡§æ‡§≤‡§ø‡§ï‡§æ‡§ï‡•ã ‡§§‡•á‡§∏‡•ç‡§∞‡•ã ‡§®‡§ó‡§∞ ‡§™‡§∞‡§ø‡§∑‡§¶‡§¨‡§æ‡§ü ‡§™‡§æ‡§∞‡§ø‡§§ ‡§Ü.‡§µ.‡•®‡•¶‡•≠‡•©‡•§‡•≠‡•™ ‡§ï‡•ã ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§ø‡§§ ‡§∞ ‡•®‡•¶‡•≠‡•™‡•§‡•≠‡•´ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡§æ‡§µ‡§ø‡§§ ‡§®‡•Ä‡§§‡§ø, ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§§‡§•‡§æ ‡§¨‡§ú‡•á‡§ü\n",
      "‡§Ö‡§æ‡§∞‡•ç‡§•‡§ø‡§ï ‡§µ‡§∞‡•ç‡§∑ ‡•®‡•¶‡•≠‡•´/‡•≠‡•¨ ‡§ï‡§æ‡•á ‡§®‡§¶‡§ø‡§ú‡§®‡•ç‡§Ø ‡§™‡§¶‡§æ‡§∞‡•ç‡§•‡§ï‡§æ‡•á ‡§â‡§§‡•ç‡§ñ‡§®‡§®‡•ç ‡§ó‡§∞‡•Ä ‡§¨‡§ø‡§ï‡•ç‡§∞‡§ø ‡§µ‡§ø‡§§‡§∞‡§£ ‡§§‡§•‡§æ ‡§Ö‡§æ‡§®‡•ç‡§§‡§∞‡§ø‡§ï ‡§®‡§ø‡§ï‡§æ‡§∏‡•Ä ‡§ó‡§∞‡•ç‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡•á ‡§¨‡§æ‡•á‡§≤‡§™‡§§‡•ç‡§∞ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§∏‡•Å‡§ö‡§®‡§æ\n",
      "‡§∏‡§ï‡•ç‡§∑‡§æ‡§∞ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡§æ‡§®‡§Æ‡§æ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä‡§¨‡§æ‡§∏‡•Ä ‡§∏‡§Æ‡•ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡§∞‡•ã‡§ï‡§æ‡§∞‡§µ‡§æ‡§≤‡§æ‡§π‡§∞‡•Å‡§ï‡•ã ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§∞ ‡§∏‡§π‡§≠‡§æ‡§ó‡§ø‡§§‡§æ‡§ï‡§æ‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§Ö‡§®‡•Å‡§∞‡§æ‡•ã‡§ß ‡§õ ‡•§ ‡§∏‡§æ‡§Æ‡•Å‡§¶‡§æ‡§Ø‡§ø‡§ï ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§π‡§∞‡•Ç‡§ï‡•ã ‡§®‡§µ‡§ø‡§ï‡§∞‡§£ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§Æ‡§æ ‡•§\n",
      "‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç, ‡•ß‡•® ‡§ï‡§æ‡§§‡§ø‡§ï ‡•§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§¶‡•á‡§µ‡•Ä ‡§≠‡§£‡•ç‡§°‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§§‡•ç‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡§§‡§æ‡§∞‡§ï‡•ã ‡§ö‡§æ‡§∞ ‡§¶‡§ø‡§µ‡§∏‡•Ä‡§Ø ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï ‡§≠‡•ç‡§∞‡§Æ‡§£‡§Æ‡§æ ‡§Ü‡§ú ‡§§‡•ç‡§Ø‡§∏‡§§‡§∞‡•ç‡§´\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text):\n",
    "    \"\"\"Clean Nepali text while preserving Devanagari punctuation.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Keep Nepali-specific characters\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English\n",
    "        r'\\u0900-\\u097F'             # Devanagari including ‡•§‡••\n",
    "        r'0-9\\u0966-\\u096F'          # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\[\\]{}\\-\\'\\\"/\\\\'   # Punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process file in chunks to avoid memory crashes\n",
    "print(\"üìñ Processing ne_corpus.txt in chunks...\")\n",
    "\n",
    "chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "buffer = []\n",
    "total_original = 0\n",
    "total_cleaned = 0\n",
    "\n",
    "try:\n",
    "    with open('ne_corpus.txt', 'r', encoding='utf-8') as infile, \\\n",
    "         open('ne_corpus_cleaned.txt', 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = infile.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_num += 1\n",
    "            total_original += len(chunk)\n",
    "            \n",
    "            # Clean chunk\n",
    "            cleaned = clean_nepali_text(chunk)\n",
    "            \n",
    "            # Filter lines (minimum 5 chars)\n",
    "            lines = [line.strip() for line in cleaned.split('\\n') \n",
    "                     if len(line.strip()) >= 5]\n",
    "            \n",
    "            # Write to output\n",
    "            if lines:\n",
    "                output = '\\n'.join(lines) + '\\n'\n",
    "                outfile.write(output)\n",
    "                total_cleaned += len(output)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_original:,} chars)...\")\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample of cleaned text ---\")\n",
    "    with open('ne_corpus_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read(500))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: ne.txt not found in current directory\")\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Still running out of memory. Try increasing chunk_size or processing smaller sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e143a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 06:41WARNING) Too many sentences are loaded! (11607422), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 06:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training complete!\n",
      "Model saved: ne_spm.model\n",
      "Vocab saved: ne_spm.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Progress tracking using file size monitoring\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.running = True\n",
    "        self.pbar = tqdm(total=100, desc=\"Training\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {elapsed}')\n",
    "        \n",
    "    def monitor(self):\n",
    "        \"\"\"Monitor training by checking if model file exists and grows\"\"\"\n",
    "        last_size = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.running:\n",
    "            time.sleep(2)  # Check every 2 seconds\n",
    "            \n",
    "            if os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_size:\n",
    "                    # File is growing, training is progressing\n",
    "                    last_size = current_size\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.pbar.set_postfix({\"size\": f\"{current_size/1024:.1f}KB\", \n",
    "                                          \"time\": f\"{elapsed:.0f}s\"})\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "# Start monitoring\n",
    "monitor = TrainingMonitor('ne_spm')\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # Train with optimized settings\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='ne_cleaned.txt',\n",
    "        model_prefix='ne_spm',\n",
    "        vocab_size=16000,\n",
    "        character_coverage=1.0,\n",
    "        model_type='bpe',\n",
    "        input_sentence_size=11607422,\n",
    "        shuffle_input_sentence=True,\n",
    "        max_sentence_length=8192,     \n",
    "        num_threads=16,\n",
    "        minloglevel=1  \n",
    "    )\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(f\"Model saved: ne_spm.model\")\n",
    "print(f\"Vocab saved: ne_spm.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62aa5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\n",
      "Tokens: ['‚ñÅ‡§®‡§Æ‡§∏‡•ç‡§§‡•á', ',', '‚ñÅ‡§Æ‡•á‡§∞‡•ã', '‚ñÅ‡§®‡§æ‡§Æ', '‚ñÅ‡§∞‡§æ‡§ú', '‚ñÅ‡§π‡•ã', '‡•§']\n",
      "Token IDs: [12984, 15861, 778, 703, 201, 106, 15848]\n",
      "Decoded: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\n",
      "\n",
      "Text: I want to book a flight to Kathmandu.\n",
      "Tokens: ['‚ñÅI', '‚ñÅw', 'ant', '‚ñÅto', '‚ñÅb', 'ook', '‚ñÅa', '‚ñÅf', 'li', 'ght', '‚ñÅto', '‚ñÅK', 'athmandu', '.']\n",
      "Token IDs: [2567, 6025, 7536, 7830, 3578, 9083, 3266, 4037, 9479, 5934, 7830, 2866, 15347, 15870]\n",
      "Decoded: I want to book a flight to Kathmandu.\n",
      "\n",
      "Text: ‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\n",
      "Tokens: ['‚ñÅ‡§Æ', '‚ñÅ‡§Ü‡§ú', '‚ñÅof', 'f', 'ice', '‚ñÅ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å', '‡•§']\n",
      "Token IDs: [26, 357, 4959, 15938, 13045, 13748, 15848]\n",
      "Decoded: ‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\n",
      "\n",
      "Text: Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\n",
      "Tokens: ['‚ñÅC', 'ust', 'om', 'er', '‚ñÅs', 'er', 'v', 'ice', '‚ñÅ‡§≤‡•á', '‚ñÅh', 'el', 'p', '‚ñÅ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ', '‡•§']\n",
      "Token IDs: [1373, 7476, 705, 1273, 3020, 1273, 15940, 13045, 353, 10733, 3228, 15906, 2131, 15848]\n",
      "Decoded: Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('ne_spm.model')\n",
    "\n",
    "# Test on mixed Nepali-English samples\n",
    "test_samples = [\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\",  # Pure Nepali\n",
    "    \"I want to book a flight to Kathmandu.\",  # Pure English\n",
    "    \"‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\",  # Code-mixed\n",
    "    \"Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\"  # Mixed domain\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {sp.encode(text)}\")\n",
    "    print(f\"Decoded: {sp.decode(sp.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b355b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
