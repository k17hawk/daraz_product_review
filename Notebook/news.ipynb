{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220296c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/lotusacharya/nepalinewsdataset\n",
      "License(s): GPL-2.0\n",
      "Downloading nepalinewsdataset.zip to /home/lang-chain/Documents/daraz_product_review/Notebook\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 18.0M/18.1M [00:01<00:00, 14.0MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.1M/18.1M [00:01<00:00, 13.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download lotusacharya/nepalinewsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder structure from: nepalinewsdataset\n",
      "--------------------------------------------------\n",
      "Found 10000 .txt files\n",
      "\n",
      "Processing files and removing duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:11<00:00, 847.54file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Summary:\n",
      "  Total files found: 10000\n",
      "  Unique files: 9999\n",
      "  Duplicate files skipped: 1\n",
      "  Estimated output size: 26.52 MB\n",
      "============================================================\n",
      "\n",
      "Writing combined content to: news.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39996/39996 [00:00<00:00, 289915.72chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully created news.txt\n",
      "üìÅ Output location: /home/lang-chain/Documents/daraz_product_review/Notebook/news.txt\n",
      "üìä Final file size: 72.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Calculate MD5 hash of a file to detect duplicates\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_and_combine_txt_files(root_folder, output_file=\"news.txt\"):\n",
    "    \"\"\"\n",
    "    Crawl through all subfolders, find .txt files, remove duplicates, and combine them\n",
    "    \n",
    "    Args:\n",
    "        root_folder: Path to the main folder (nepalinewsdataset)\n",
    "        output_file: Name of the output combined file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store file hashes and content (to detect duplicates)\n",
    "    seen_files = {}\n",
    "    txt_files = []\n",
    "    total_size = 0\n",
    "    \n",
    "    print(f\"Scanning folder structure from: {root_folder}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # First pass: Collect all .txt files with progress\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                txt_files.append(filepath)\n",
    "\n",
    "    \n",
    "    print(f\"Found {len(txt_files)} .txt files\")\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(\"No .txt files found!\")\n",
    "        return\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    unique_files = 0\n",
    "    duplicate_files = 0\n",
    "    combined_content = []\n",
    "    \n",
    "    print(\"\\nProcessing files and removing duplicates...\")\n",
    "    \n",
    "    with tqdm(total=len(txt_files), desc=\"Processing\", unit=\"file\") as pbar:\n",
    "        for filepath in txt_files:\n",
    "            try:\n",
    "                # Get file hash to check for duplicates\n",
    "                file_hash = get_file_hash(filepath)\n",
    "                \n",
    "                if file_hash and file_hash not in seen_files:\n",
    "                    # Read file content\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read().strip()\n",
    "                    \n",
    "                    # Only add if file has content\n",
    "                    if content:\n",
    "                        # Add metadata about source file\n",
    "                        relative_path = os.path.relpath(filepath, root_folder)\n",
    "                        combined_content.append(f\"\\n\\n{'='*80}\\n\")\n",
    "                        combined_content.append(f\"Source: {relative_path}\\n\")\n",
    "                        combined_content.append(f\"{'='*80}\\n\\n\")\n",
    "                        combined_content.append(content)\n",
    "                        \n",
    "                        seen_files[file_hash] = True\n",
    "                        unique_files += 1\n",
    "                        total_size += len(content)\n",
    "                    else:\n",
    "                        print(f\"\\nSkipping empty file: {filepath}\")\n",
    "                elif file_hash:\n",
    "                    duplicate_files += 1\n",
    "                    \n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if utf-8 fails\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "                        content = f.read().strip()\n",
    "                    \n",
    "                    if content:\n",
    "                        relative_path = os.path.relpath(filepath, root_folder)\n",
    "                        combined_content.append(f\"\\n\\n{'='*80}\\n\")\n",
    "                        combined_content.append(f\"Source: {relative_path}\\n\")\n",
    "                        combined_content.append(f\"{'='*80}\\n\\n\")\n",
    "                        combined_content.append(content)\n",
    "                        \n",
    "                        seen_files[file_hash] = True\n",
    "                        unique_files += 1\n",
    "                        total_size += len(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nCould not read {filepath} with any encoding: {e}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {filepath}: {e}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            # Small sleep to prevent CPU overuse\n",
    "            time.sleep(0.001)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  Total files found: {len(txt_files)}\")\n",
    "    print(f\"  Unique files: {unique_files}\")\n",
    "    print(f\"  Duplicate files skipped: {duplicate_files}\")\n",
    "    print(f\"  Estimated output size: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Write combined content to output file\n",
    "    if combined_content:\n",
    "        print(f\"Writing combined content to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            # Write header\n",
    "            outfile.write(f\"{'='*80}\\n\")\n",
    "            outfile.write(f\"COMBINED NEPALI NEWS DATASET\\n\")\n",
    "            outfile.write(f\"Generated from: {root_folder}\\n\")\n",
    "            outfile.write(f\"Total unique articles: {unique_files}\\n\")\n",
    "            outfile.write(f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            outfile.write(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Write all content\n",
    "            for content_chunk in tqdm(combined_content, desc=\"Writing to file\", unit=\"chunk\"):\n",
    "                outfile.write(content_chunk)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully created {output_file}\")\n",
    "        print(f\"üìÅ Output location: {os.path.abspath(output_file)}\")\n",
    "        \n",
    "        # Get final file size\n",
    "        output_size = os.path.getsize(output_file) / (1024*1024)\n",
    "        print(f\"üìä Final file size: {output_size:.2f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå No content to write!\")\n",
    "\n",
    "def main():\n",
    "    # Set your main folder path\n",
    "    root_folder = \"nepalinewsdataset\"  # Change this if needed\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(root_folder):\n",
    "        print(f\"Error: Folder '{root_folder}' not found!\")\n",
    "        print(\"Please make sure you're running this script from the correct directory.\")\n",
    "        return\n",
    "    \n",
    "    # Set output file name\n",
    "    output_file = \"news.txt\"\n",
    "    \n",
    "    # Check if output file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        response = input(f\"{output_file} already exists. Overwrite? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "    \n",
    "    # Run the crawler and combiner\n",
    "    try:\n",
    "        crawl_and_combine_txt_files(root_folder, output_file)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Process interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install tqdm if not installed\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        print(\"Installing required package: tqdm\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([\"pip\", \"install\", \"tqdm\"])\n",
    "        from tqdm import tqdm\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc38be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Processing news.txt in chunks...\n",
      "  Processed 10 chunks (10,485,760 chars)...\n",
      "  Processed 20 chunks (20,971,520 chars)...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 29,798,999 chars\n",
      "Cleaned: 27,530,783 chars\n",
      "Removed: 2,268,216 chars\n",
      "\n",
      "--- Sample of cleaned text ---\n",
      "‚ùå Error: news.txt not found in current directory\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text):\n",
    "    \"\"\"Clean Nepali text while preserving Devanagari punctuation.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Keep Nepali-specific characters\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English\n",
    "        r'\\u0900-\\u097F'             # Devanagari including ‡•§‡••\n",
    "        r'0-9\\u0966-\\u096F'          # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\[\\]{}\\-\\'\\\"/\\\\'   # Punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process file in chunks to avoid memory crashes\n",
    "print(\"üìñ Processing news.txt in chunks...\")\n",
    "\n",
    "chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "buffer = []\n",
    "total_original = 0\n",
    "total_cleaned = 0\n",
    "\n",
    "try:\n",
    "    with open('news.txt', 'r', encoding='utf-8') as infile, \\\n",
    "         open('new_cleaned.txt', 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = infile.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_num += 1\n",
    "            total_original += len(chunk)\n",
    "            \n",
    "            # Clean chunk\n",
    "            cleaned = clean_nepali_text(chunk)\n",
    "            \n",
    "            # Filter lines (minimum 5 chars)\n",
    "            lines = [line.strip() for line in cleaned.split('\\n') \n",
    "                     if len(line.strip()) >= 5]\n",
    "            \n",
    "            # Write to output\n",
    "            if lines:\n",
    "                output = '\\n'.join(lines) + '\\n'\n",
    "                outfile.write(output)\n",
    "                total_cleaned += len(output)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_original:,} chars)...\")\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample of cleaned text ---\")\n",
    "    with open('news_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read(500))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: news.txt not found in current directory\")\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Still running out of memory. Try increasing chunk_size or processing smaller sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624e137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing wikipedia_ncc_corpus.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6610859it [00:29, 224942.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing new_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97988it [00:00, 185291.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 6,708,847\n",
      "Unique lines written: 6,707,124\n",
      "Output saved to: news_wikipedia_ncc_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'wikipedia_ncc_corpus.txt'\n",
    "file2 = 'new_cleaned.txt'\n",
    "output_file = 'news_wikipedia_ncc_corpus.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'facebook/cc100' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CC-100 English corpus (max 200,000 lines)...\n",
      "Error: Dataset 'facebook/cc100' doesn't exist on the Hub or cannot be accessed.\n",
      "\n",
      "Trying alternative method...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc46d0038d046a39f6fa7e54bf7deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0383a3f4cb4884a399f56f89ca4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading C4 (alternative): 199999it [00:45, 4353.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded C4 English: 200,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cc100_en_200k.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_cc100_english(output_file='cc100_en.txt', max_lines=500000):\n",
    "    \"\"\"\n",
    "    Download CC-100 English corpus\n",
    "    \"\"\"\n",
    "    print(f\"Downloading CC-100 English corpus (max {max_lines:,} lines)...\")\n",
    "    \n",
    "    try:\n",
    "        # CC-100 is available on Hugging Face\n",
    "        dataset = load_dataset(\n",
    "            'facebook/cc100',\n",
    "            lang='en',\n",
    "            streaming=True,  # Critical for large datasets\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        lines_written = 0\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in tqdm(dataset['train'], desc=\"Downloading\"):\n",
    "                text = example.get('text', '').strip()\n",
    "                \n",
    "                if text:\n",
    "                    # Basic filtering\n",
    "                    words = text.split()\n",
    "                    if 3 <= len(words) <= 512:  # Reasonable length\n",
    "                        f.write(text + '\\n')\n",
    "                        lines_written += 1\n",
    "                        \n",
    "                        if lines_written >= max_lines:\n",
    "                            break\n",
    "        \n",
    "        # Get file size\n",
    "        size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "        \n",
    "        print(f\"\\n‚úì Downloaded {lines_written:,} lines\")\n",
    "        print(f\"‚úì File size: {size_mb:.1f} MB\")\n",
    "        print(f\"‚úì Saved to: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nTrying alternative method...\")\n",
    "        return download_cc100_alternative(output_file, max_lines)\n",
    "\n",
    "def download_cc100_alternative(output_file, max_lines):\n",
    "    \"\"\"Alternative CC-100 download method\"\"\"\n",
    "    try:\n",
    "        # Sometimes CC-100 is in different format\n",
    "        dataset = load_dataset('allenai/c4', 'en', streaming=True)\n",
    "        \n",
    "        lines_written = 0\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in tqdm(dataset['train'], desc=\"Downloading C4 (alternative)\"):\n",
    "                text = example.get('text', '').strip()\n",
    "                if text:\n",
    "                    f.write(text + '\\n')\n",
    "                    lines_written += 1\n",
    "                    if lines_written >= max_lines:\n",
    "                        break\n",
    "        \n",
    "        print(f\"‚úì Downloaded C4 English: {lines_written:,} lines\")\n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Alternative failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download reasonable amount\n",
    "download_cc100_english('cc100_en_200k.txt', max_lines=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fed30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Cleaning cc100_en_200k.txt...\n",
      "  Processed 10 chunks...\n",
      "  Processed 20 chunks...\n",
      "  Processed 30 chunks...\n",
      "  Processed 40 chunks...\n",
      "  Processed 50 chunks...\n",
      "  Processed 60 chunks...\n",
      "  Processed 70 chunks...\n",
      "  Processed 80 chunks...\n",
      "  Processed 90 chunks...\n",
      "  Processed 100 chunks...\n",
      "  Processed 110 chunks...\n",
      "  Processed 120 chunks...\n",
      "  Processed 130 chunks...\n",
      "  Processed 140 chunks...\n",
      "  Processed 150 chunks...\n",
      "  Processed 160 chunks...\n",
      "  Processed 170 chunks...\n",
      "  Processed 180 chunks...\n",
      "  Processed 190 chunks...\n",
      "  Processed 200 chunks...\n",
      "  Processed 210 chunks...\n",
      "  Processed 220 chunks...\n",
      "  Processed 230 chunks...\n",
      "  Processed 240 chunks...\n",
      "  Processed 250 chunks...\n",
      "  Processed 260 chunks...\n",
      "  Processed 270 chunks...\n",
      "  Processed 280 chunks...\n",
      "  Processed 290 chunks...\n",
      "  Processed 300 chunks...\n",
      "  Processed 310 chunks...\n",
      "  Processed 320 chunks...\n",
      "  Processed 330 chunks...\n",
      "  Processed 340 chunks...\n",
      "  Processed 350 chunks...\n",
      "  Processed 360 chunks...\n",
      "  Processed 370 chunks...\n",
      "  Processed 380 chunks...\n",
      "  Processed 390 chunks...\n",
      "  Processed 400 chunks...\n",
      "  Processed 410 chunks...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 431,219,894 chars\n",
      "Cleaned: 287,856,338 chars\n",
      "Removed: 143,363,556 chars\n",
      "\n",
      "--- Sample ---\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat select\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_english_text_simple(text):\n",
    "    \"\"\"Simple English text cleaner similar to your Nepali cleaner.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Step 2: Keep only English characters, digits, and basic punctuation\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English letters\n",
    "        r'0-9'                       # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\-_\\'\"'            # Basic punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    # Step 4: Remove lines that are too short or too long\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if 20 <= len(line) <= 500:  # Reasonable line length\n",
    "            # Check if has at least 3 words\n",
    "            if len(line.split()) >= 3:\n",
    "                filtered_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "# Process file in chunks (identical to your Nepali processor)\n",
    "def clean_english_file_simple(input_file, output_file, chunk_size=1024*1024):\n",
    "    \"\"\"Simple English file cleaner with chunk processing.\"\"\"\n",
    "    \n",
    "    print(f\"üìñ Cleaning {input_file}...\")\n",
    "    \n",
    "    total_original = 0\n",
    "    total_cleaned = 0\n",
    "    chunk_num = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
    "             open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            \n",
    "            while True:\n",
    "                chunk = infile.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                chunk_num += 1\n",
    "                total_original += len(chunk)\n",
    "                \n",
    "                # Clean chunk\n",
    "                cleaned = clean_english_text_simple(chunk)\n",
    "                \n",
    "                # Write if not empty\n",
    "                if cleaned.strip():\n",
    "                    outfile.write(cleaned + '\\n')\n",
    "                    total_cleaned += len(cleaned)\n",
    "                \n",
    "                # Progress\n",
    "                if chunk_num % 10 == 0:\n",
    "                    print(f\"  Processed {chunk_num} chunks...\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: {input_file} not found\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample ---\")\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        sample = ''.join([f.readline() for _ in range(3)])\n",
    "        print(sample[:500])\n",
    "\n",
    "# Usage (just like your Nepali code)\n",
    "if __name__ == \"__main__\":\n",
    "    clean_english_file_simple(\n",
    "        input_file='cc100_en_200k.txt',\n",
    "        output_file='cc100_en_cleaned.txt',\n",
    "        chunk_size=1024*1024  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e19d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing news_wikipedia_ncc_corpus.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6707124it [00:29, 225239.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cc100_en_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1515908it [00:02, 536323.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 8,223,032\n",
      "Unique lines written: 8,222,518\n",
      "Output saved to: eng_news_wikipedia_ncc_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'news_wikipedia_ncc_corpus.txt'\n",
    "file2 = 'cc100_en_cleaned.txt'\n",
    "output_file = 'eng_news_wikipedia_ncc_corpus.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c802a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for eng_news_wikipedia_ncc_corpus (8.2M lines)...\n",
      "\n",
      "üìä Analyzing corpus composition...\n",
      "   File size: 3.64 GB\n",
      "   English-only lines: 0.0%\n",
      "   Nepali-only lines: 90.4%\n",
      "   Mixed lines: 9.6%\n",
      "   ‚Üí Optimizing for bilingual tokenization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8.2M corpus:   0%|          | 0/100 [00:00<?, ]"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "print(\"Starting training for eng_news_wikipedia_ncc_corpus (8.2M lines)...\")\n",
    "\n",
    "# ============ CORPUS ANALYSIS ============\n",
    "print(\"\\nüìä Analyzing corpus composition...\")\n",
    "try:\n",
    "    # Check if file exists\n",
    "    input_file = 'eng_news_wikipedia_ncc_corpus.txt'\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ùå ERROR: Input file not found: {input_file}\")\n",
    "        print(\"   Please check the file path and try again.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    print(f\"   File size: {file_size / (1024*1024*1024):.2f} GB\")\n",
    "    \n",
    "    # Sample first 10000 lines safely\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        sample_lines = list(islice(f, min(10000, 8222518)))\n",
    "    \n",
    "    devanagari_pattern = re.compile(r'[\\u0900-\\u097F]')  # Nepali/Devanagari\n",
    "    latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "    \n",
    "    eng_only = sum(1 for line in sample_lines if latin_pattern.search(line) and not devanagari_pattern.search(line))\n",
    "    nep_only = sum(1 for line in sample_lines if devanagari_pattern.search(line) and not latin_pattern.search(line))\n",
    "    mixed = sum(1 for line in sample_lines if devanagari_pattern.search(line) and latin_pattern.search(line))\n",
    "    \n",
    "    print(f\"   English-only lines: {eng_only/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   Nepali-only lines: {nep_only/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   Mixed lines: {mixed/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   ‚Üí Optimizing for bilingual tokenization...\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not analyze corpus: {e}\")\n",
    "    print(f\"   ‚Üí Proceeding with mixed language settings...\\n\")\n",
    "\n",
    "# BETTER PROGRESS TRACKING - Time-based since SentencePiece doesn't output progress\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix, total_minutes=180):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.vocab_file = f\"{model_prefix}.vocab\"\n",
    "        self.running = True\n",
    "        self.start_time = time.time()\n",
    "        self.total_minutes = total_minutes\n",
    "        \n",
    "        # Create a progress bar for time estimation\n",
    "        self.pbar = tqdm(total=100, desc=\"Training 8.2M corpus\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {postfix}]')\n",
    "        \n",
    "    def monitor(self):\n",
    "        \"\"\"Time-based monitoring since SentencePiece doesn't output progress\"\"\"\n",
    "        stages = [\n",
    "            (0, \"Initializing\"),\n",
    "            (5, \"Reading corpus\"),\n",
    "            (15, \"Computing vocab\"),\n",
    "            (40, \"Training BPE\"),\n",
    "            (70, \"Optimizing\"),\n",
    "            (90, \"Finalizing\"),\n",
    "            (100, \"Complete\")\n",
    "        ]\n",
    "        \n",
    "        last_file_size = 0\n",
    "        file_check_count = 0\n",
    "        \n",
    "        while self.running:\n",
    "            elapsed_seconds = time.time() - self.start_time\n",
    "            elapsed_minutes = elapsed_seconds / 60\n",
    "            \n",
    "            # Estimate progress based on time (max 99% until done)\n",
    "            progress = min(99, (elapsed_minutes / self.total_minutes) * 100)\n",
    "            \n",
    "            # Check if model file exists and is growing (occasionally)\n",
    "            file_check_count += 1\n",
    "            if file_check_count % 10 == 0 and os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_file_size:\n",
    "                    last_file_size = current_size\n",
    "                    # File is growing, we're likely at 70-90% stage\n",
    "                    progress = max(progress, 70)\n",
    "            \n",
    "            # Determine current stage\n",
    "            current_stage = stages[-1][1]\n",
    "            for stage_progress, stage_name in stages:\n",
    "                if progress >= stage_progress:\n",
    "                    current_stage = stage_name\n",
    "            \n",
    "            # Update progress bar\n",
    "            self.pbar.n = int(progress)\n",
    "            self.pbar.set_postfix({\n",
    "                \"stage\": current_stage,\n",
    "                \"elapsed\": f\"{elapsed_minutes:.0f}m\",\n",
    "                \"est_remain\": f\"{max(0, self.total_minutes - elapsed_minutes):.0f}m\"\n",
    "            })\n",
    "            self.pbar.refresh()\n",
    "            \n",
    "            time.sleep(5)  # Update every 5 seconds\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join(timeout=2)\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.set_postfix({\"stage\": \"Complete!\", \"elapsed\": f\"{(time.time() - self.start_time)/60:.0f}m\"})\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "# Start monitoring - estimate 3 hours for 6M lines\n",
    "monitor = TrainingMonitor('eng_nep_8m_spm', total_minutes=180)\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # FIXED PARAMETERS with valid SentencePiece arguments\n",
    "    print(\"\\nüîß Training with optimized parameters for large corpus...\")\n",
    "    print(\"   Estimated time: 2-3 hours for 6M lines\")\n",
    "    \n",
    "    # Use keyword arguments instead of building arg_string to avoid issues\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='eng_news_wikipedia_ncc_corpus.txt',\n",
    "        model_prefix='eng_nep_8m_spm',\n",
    "        vocab_size=50000,  # Changed to match what's printed later\n",
    "        character_coverage=0.9999,\n",
    "        model_type='bpe',\n",
    "\n",
    "        # Text splitting parameters\n",
    "        split_by_whitespace=True,\n",
    "        split_by_unicode_script=True,  # Changed to True for better language separation\n",
    "        split_digits=False,\n",
    "        byte_fallback=False,\n",
    "\n",
    "        # Token length\n",
    "        max_sentencepiece_length=80,\n",
    "\n",
    "        # Corpus handling - using 6M lines as specified\n",
    "        input_sentence_size=6000000,\n",
    "        shuffle_input_sentence=True,\n",
    "        \n",
    "        # Hardware optimization\n",
    "        num_threads=8,  # Reduced for stability\n",
    "        \n",
    "        # Normalization (FIXED parameter name)\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "        remove_extra_whitespace=True,  # FIXED: removed 's' from 'whitespaces'\n",
    "        add_dummy_prefix=True,\n",
    "\n",
    "        # Special tokens as comma-separated string\n",
    "        user_defined_symbols='<ENG>,<NEP>,<MIXED>,<NUM>,<DATE>,<URL>,<EMAIL>',\n",
    "\n",
    "        # Token IDs\n",
    "        unk_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        pad_id=-1,\n",
    "        \n",
    "        # Additional optimization for large corpus\n",
    "        max_sentence_length=8192,\n",
    "        seed_sentencepiece_size=3000000,\n",
    "        training_time=10,  # seconds per merge operation\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    monitor.stop()\n",
    "    exit(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"1. Check if input file exists and is readable\")\n",
    "    print(\"2. Ensure you have enough RAM (6M lines needs ~4-6GB)\")\n",
    "    print(\"3. Try reducing vocab_size to 32000\")\n",
    "    print(\"4. Try reducing num_threads to 4\")\n",
    "    print(\"5. Check file encoding (should be UTF-8)\")\n",
    "    \n",
    "    # Try with smaller parameters\n",
    "    print(\"\\nüîÑ Trying with reduced parameters...\")\n",
    "    try:\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input='eng_news_wikipedia_ncc_corpus.txt',\n",
    "            model_prefix='eng_nep_8m_spm_small',\n",
    "            vocab_size=32000,\n",
    "            model_type='bpe',\n",
    "            num_threads=4,\n",
    "            input_sentence_size=3000000,\n",
    "            character_coverage=0.999,\n",
    "        )\n",
    "        print(\"\\n‚úì Training completed with reduced parameters!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"\\n‚ùå Reduced training also failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check which model was created\n",
    "if os.path.exists('eng_nep_8m_spm.model'):\n",
    "    model_file = 'eng_nep_8m_spm.model'\n",
    "    print(f\"üìä Corpus size processed: 6,000,000 lines\")\n",
    "    print(f\"ü§ñ Model saved: eng_nep_8m_spm.model\")\n",
    "    print(f\"üìö Vocab saved: eng_nep_8m_spm.vocab\")\n",
    "    print(f\"üéØ Vocab size: 50,000 tokens\")\n",
    "elif os.path.exists('eng_nep_8m_spm_small.model'):\n",
    "    model_file = 'eng_nep_8m_spm_small.model'\n",
    "    print(f\"üìä Corpus size processed: 3,000,000 lines\")\n",
    "    print(f\"ü§ñ Model saved: eng_nep_8m_spm_small.model\")\n",
    "    print(f\"üìö Vocab saved: eng_nep_8m_spm_small.vocab\")\n",
    "    print(f\"üéØ Vocab size: 32,000 tokens\")\n",
    "else:\n",
    "    print(\"‚ùå No model file was created\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============ COMPREHENSIVE TESTING ============\n",
    "print(\"\\nüß™ Testing tokenizer performance...\")\n",
    "\n",
    "try:\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "    \n",
    "    test_cases = [\n",
    "        # English sentences\n",
    "        (\"English\", \"Breaking news: Nepal's government announced new policies today.\"),\n",
    "        (\"English\", \"Wikipedia is a free online encyclopedia with millions of articles.\"),\n",
    "        \n",
    "        # Nepali sentences  \n",
    "        (\"Nepali\", \"‡§Ü‡§ú‡§ï‡•ã ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞: ‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§®‡§Ø‡§æ‡§Å ‡§®‡•Ä‡§§‡§ø ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ó‡§∞‡•ç‡§Ø‡•ã‡•§\"),\n",
    "        (\"Nepali\", \"‡§µ‡§ø‡§ï‡§ø‡§™‡§ø‡§°‡§ø‡§Ø‡§æ ‡§è‡§ï ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡•ã‡§∂ ‡§π‡•ã‡•§\"),\n",
    "        \n",
    "        # Mixed content\n",
    "        (\"Mixed\", \"Nepal ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á Wikipedia ‡§Æ‡§æ article ‡§≤‡•á‡§ñ‡•ç‡§Ø‡•ã‡•§\"),\n",
    "        (\"Mixed\", \"‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç‡§Æ‡§æ temperature ‡§Ü‡§ú 25¬∞C ‡§õ according to weather report‡•§\"),\n",
    "        \n",
    "        # Numbers and dates\n",
    "        (\"Mixed\", \"‡•®‡•¶‡•®‡•™ ‡§∏‡§æ‡§≤‡§Æ‡§æ Nepal ‡§Æ‡§æ ‡•≠ ‡§ï‡§∞‡•ã‡§° population ‡§õ‡•§\"),\n",
    "        (\"English\", \"The population of Nepal in 2024 is approximately 30 million.\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìà Vocabulary size: {sp.vocab_size():,} tokens\")\n",
    "    print(f\"üî§ Unknown token: {sp.id_to_piece(sp.unk_id())}\")\n",
    "    print(f\"üî§ Begin token: {sp.id_to_piece(sp.bos_id())}\")\n",
    "    print(f\"üî§ End token: {sp.id_to_piece(sp.eos_id())}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (lang, sentence) in enumerate(test_cases, 1):\n",
    "        tokens = sp.encode(sentence, out_type=str)\n",
    "        token_ids = sp.encode(sentence, out_type=int)\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        print(f\"\\n{i}. [{lang}] '{sentence[:50]}{'...' if len(sentence) > 50 else ''}'\")\n",
    "        print(f\"   ‚Üí {token_count} tokens\")\n",
    "        print(f\"   ‚Üí Tokens: {' '.join(tokens[:12])}{'...' if token_count > 12 else ''}\")\n",
    "        \n",
    "        # Show first few token IDs for reference\n",
    "        if i == 1:\n",
    "            print(f\"   ‚Üí First 5 token IDs: {token_ids[:5]}\")\n",
    "    \n",
    "    # Test some basic operations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ Additional tests:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Test encoding/decoding\n",
    "    test_text = \"Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á\"\n",
    "    encoded = sp.encode(test_text, out_type=int)\n",
    "    decoded = sp.decode(encoded)\n",
    "    print(f\"Encode/decode test:\")\n",
    "    print(f\"  Original: '{test_text}'\")\n",
    "    print(f\"  Encoded: {encoded}\")\n",
    "    print(f\"  Decoded: '{decoded}'\")\n",
    "    print(f\"  Match: {'‚úì' if test_text == decoded else '‚úó'}\")\n",
    "    \n",
    "    # Show some vocabulary samples\n",
    "    print(f\"\\nSample vocabulary (first 10):\")\n",
    "    for i in range(min(10, sp.vocab_size())):\n",
    "        piece = sp.id_to_piece(i)\n",
    "        if piece.startswith('‚ñÅ'):\n",
    "            print(f\"  {i:4d}: '{piece}' (prefix)\")\n",
    "        else:\n",
    "            print(f\"  {i:4d}: '{piece}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Testing failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Ready for use with your corpus!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"  import sentencepiece as spm\")\n",
    "print(f\"  sp = spm.SentencePieceProcessor(model_file='{model_file}')\")\n",
    "print(\"  tokens = sp.encode('Your text here')\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0baed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 2 2 2 5 1 8\n",
    "\n",
    "2 0 0 0 0 0 0\n",
    "\n",
    "6 0 0 0 0 0 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
