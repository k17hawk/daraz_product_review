{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220296c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/lotusacharya/nepalinewsdataset\n",
      "License(s): GPL-2.0\n",
      "Downloading nepalinewsdataset.zip to /home/lang-chain/Documents/daraz_product_review/Notebook\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 18.0M/18.1M [00:01<00:00, 14.0MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.1M/18.1M [00:01<00:00, 13.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download lotusacharya/nepalinewsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder structure from: nepalinewsdataset\n",
      "--------------------------------------------------\n",
      "Found 10000 .txt files\n",
      "\n",
      "Processing files and removing duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:11<00:00, 847.54file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Summary:\n",
      "  Total files found: 10000\n",
      "  Unique files: 9999\n",
      "  Duplicate files skipped: 1\n",
      "  Estimated output size: 26.52 MB\n",
      "============================================================\n",
      "\n",
      "Writing combined content to: news.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39996/39996 [00:00<00:00, 289915.72chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully created news.txt\n",
      "üìÅ Output location: /home/lang-chain/Documents/daraz_product_review/Notebook/news.txt\n",
      "üìä Final file size: 72.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Calculate MD5 hash of a file to detect duplicates\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_and_combine_txt_files(root_folder, output_file=\"news.txt\"):\n",
    "    \"\"\"\n",
    "    Crawl through all subfolders, find .txt files, remove duplicates, and combine them\n",
    "    \n",
    "    Args:\n",
    "        root_folder: Path to the main folder (nepalinewsdataset)\n",
    "        output_file: Name of the output combined file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store file hashes and content (to detect duplicates)\n",
    "    seen_files = {}\n",
    "    txt_files = []\n",
    "    total_size = 0\n",
    "    \n",
    "    print(f\"Scanning folder structure from: {root_folder}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # First pass: Collect all .txt files with progress\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                txt_files.append(filepath)\n",
    "\n",
    "    \n",
    "    print(f\"Found {len(txt_files)} .txt files\")\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(\"No .txt files found!\")\n",
    "        return\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    unique_files = 0\n",
    "    duplicate_files = 0\n",
    "    combined_content = []\n",
    "    \n",
    "    print(\"\\nProcessing files and removing duplicates...\")\n",
    "    \n",
    "    with tqdm(total=len(txt_files), desc=\"Processing\", unit=\"file\") as pbar:\n",
    "        for filepath in txt_files:\n",
    "            try:\n",
    "                # Get file hash to check for duplicates\n",
    "                file_hash = get_file_hash(filepath)\n",
    "                \n",
    "                if file_hash and file_hash not in seen_files:\n",
    "                    # Read file content\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read().strip()\n",
    "                    \n",
    "                    # Only add if file has content\n",
    "                    if content:\n",
    "                        # Add metadata about source file\n",
    "                        relative_path = os.path.relpath(filepath, root_folder)\n",
    "                        combined_content.append(f\"\\n\\n{'='*80}\\n\")\n",
    "                        combined_content.append(f\"Source: {relative_path}\\n\")\n",
    "                        combined_content.append(f\"{'='*80}\\n\\n\")\n",
    "                        combined_content.append(content)\n",
    "                        \n",
    "                        seen_files[file_hash] = True\n",
    "                        unique_files += 1\n",
    "                        total_size += len(content)\n",
    "                    else:\n",
    "                        print(f\"\\nSkipping empty file: {filepath}\")\n",
    "                elif file_hash:\n",
    "                    duplicate_files += 1\n",
    "                    \n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if utf-8 fails\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "                        content = f.read().strip()\n",
    "                    \n",
    "                    if content:\n",
    "                        relative_path = os.path.relpath(filepath, root_folder)\n",
    "                        combined_content.append(f\"\\n\\n{'='*80}\\n\")\n",
    "                        combined_content.append(f\"Source: {relative_path}\\n\")\n",
    "                        combined_content.append(f\"{'='*80}\\n\\n\")\n",
    "                        combined_content.append(content)\n",
    "                        \n",
    "                        seen_files[file_hash] = True\n",
    "                        unique_files += 1\n",
    "                        total_size += len(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nCould not read {filepath} with any encoding: {e}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {filepath}: {e}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "            # Small sleep to prevent CPU overuse\n",
    "            time.sleep(0.001)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  Total files found: {len(txt_files)}\")\n",
    "    print(f\"  Unique files: {unique_files}\")\n",
    "    print(f\"  Duplicate files skipped: {duplicate_files}\")\n",
    "    print(f\"  Estimated output size: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Write combined content to output file\n",
    "    if combined_content:\n",
    "        print(f\"Writing combined content to: {output_file}\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            # Write header\n",
    "            outfile.write(f\"{'='*80}\\n\")\n",
    "            outfile.write(f\"COMBINED NEPALI NEWS DATASET\\n\")\n",
    "            outfile.write(f\"Generated from: {root_folder}\\n\")\n",
    "            outfile.write(f\"Total unique articles: {unique_files}\\n\")\n",
    "            outfile.write(f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            outfile.write(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Write all content\n",
    "            for content_chunk in tqdm(combined_content, desc=\"Writing to file\", unit=\"chunk\"):\n",
    "                outfile.write(content_chunk)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully created {output_file}\")\n",
    "        print(f\"üìÅ Output location: {os.path.abspath(output_file)}\")\n",
    "        \n",
    "        # Get final file size\n",
    "        output_size = os.path.getsize(output_file) / (1024*1024)\n",
    "        print(f\"üìä Final file size: {output_size:.2f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå No content to write!\")\n",
    "\n",
    "def main():\n",
    "    # Set your main folder path\n",
    "    root_folder = \"nepalinewsdataset\"  # Change this if needed\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(root_folder):\n",
    "        print(f\"Error: Folder '{root_folder}' not found!\")\n",
    "        print(\"Please make sure you're running this script from the correct directory.\")\n",
    "        return\n",
    "    \n",
    "    # Set output file name\n",
    "    output_file = \"news.txt\"\n",
    "    \n",
    "    # Check if output file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        response = input(f\"{output_file} already exists. Overwrite? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "    \n",
    "    # Run the crawler and combiner\n",
    "    try:\n",
    "        crawl_and_combine_txt_files(root_folder, output_file)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Process interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install tqdm if not installed\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        print(\"Installing required package: tqdm\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([\"pip\", \"install\", \"tqdm\"])\n",
    "        from tqdm import tqdm\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc38be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Processing news.txt in chunks...\n",
      "  Processed 10 chunks (10,485,760 chars)...\n",
      "  Processed 20 chunks (20,971,520 chars)...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 29,798,999 chars\n",
      "Cleaned: 27,530,783 chars\n",
      "Removed: 2,268,216 chars\n",
      "\n",
      "--- Sample of cleaned text ---\n",
      "‚ùå Error: news.txt not found in current directory\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text):\n",
    "    \"\"\"Clean Nepali text while preserving Devanagari punctuation.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Keep Nepali-specific characters\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English\n",
    "        r'\\u0900-\\u097F'             # Devanagari including ‡•§‡••\n",
    "        r'0-9\\u0966-\\u096F'          # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\[\\]{}\\-\\'\\\"/\\\\'   # Punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process file in chunks to avoid memory crashes\n",
    "print(\"üìñ Processing news.txt in chunks...\")\n",
    "\n",
    "chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "buffer = []\n",
    "total_original = 0\n",
    "total_cleaned = 0\n",
    "\n",
    "try:\n",
    "    with open('news.txt', 'r', encoding='utf-8') as infile, \\\n",
    "         open('new_cleaned.txt', 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = infile.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_num += 1\n",
    "            total_original += len(chunk)\n",
    "            \n",
    "            # Clean chunk\n",
    "            cleaned = clean_nepali_text(chunk)\n",
    "            \n",
    "            # Filter lines (minimum 5 chars)\n",
    "            lines = [line.strip() for line in cleaned.split('\\n') \n",
    "                     if len(line.strip()) >= 5]\n",
    "            \n",
    "            # Write to output\n",
    "            if lines:\n",
    "                output = '\\n'.join(lines) + '\\n'\n",
    "                outfile.write(output)\n",
    "                total_cleaned += len(output)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_original:,} chars)...\")\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample of cleaned text ---\")\n",
    "    with open('news_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read(500))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: news.txt not found in current directory\")\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Still running out of memory. Try increasing chunk_size or processing smaller sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624e137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing wikipedia_ncc_corpus.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6610859it [00:29, 224942.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing new_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97988it [00:00, 185291.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 6,708,847\n",
      "Unique lines written: 6,707,124\n",
      "Output saved to: news_wikipedia_ncc_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'wikipedia_ncc_corpus.txt'\n",
    "file2 = 'new_cleaned.txt'\n",
    "output_file = 'news_wikipedia_ncc_corpus.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'facebook/cc100' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CC-100 English corpus (max 200,000 lines)...\n",
      "Error: Dataset 'facebook/cc100' doesn't exist on the Hub or cannot be accessed.\n",
      "\n",
      "Trying alternative method...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc46d0038d046a39f6fa7e54bf7deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0383a3f4cb4884a399f56f89ca4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading C4 (alternative): 199999it [00:45, 4353.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded C4 English: 200,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cc100_en_200k.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_cc100_english(output_file='cc100_en.txt', max_lines=500000):\n",
    "    \"\"\"\n",
    "    Download CC-100 English corpus\n",
    "    \"\"\"\n",
    "    print(f\"Downloading CC-100 English corpus (max {max_lines:,} lines)...\")\n",
    "    \n",
    "    try:\n",
    "        # CC-100 is available on Hugging Face\n",
    "        dataset = load_dataset(\n",
    "            'facebook/cc100',\n",
    "            lang='en',\n",
    "            streaming=True,  # Critical for large datasets\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        lines_written = 0\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in tqdm(dataset['train'], desc=\"Downloading\"):\n",
    "                text = example.get('text', '').strip()\n",
    "                \n",
    "                if text:\n",
    "                    # Basic filtering\n",
    "                    words = text.split()\n",
    "                    if 3 <= len(words) <= 512:  # Reasonable length\n",
    "                        f.write(text + '\\n')\n",
    "                        lines_written += 1\n",
    "                        \n",
    "                        if lines_written >= max_lines:\n",
    "                            break\n",
    "        \n",
    "        # Get file size\n",
    "        size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "        \n",
    "        print(f\"\\n‚úì Downloaded {lines_written:,} lines\")\n",
    "        print(f\"‚úì File size: {size_mb:.1f} MB\")\n",
    "        print(f\"‚úì Saved to: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nTrying alternative method...\")\n",
    "        return download_cc100_alternative(output_file, max_lines)\n",
    "\n",
    "def download_cc100_alternative(output_file, max_lines):\n",
    "    \"\"\"Alternative CC-100 download method\"\"\"\n",
    "    try:\n",
    "        # Sometimes CC-100 is in different format\n",
    "        dataset = load_dataset('allenai/c4', 'en', streaming=True)\n",
    "        \n",
    "        lines_written = 0\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in tqdm(dataset['train'], desc=\"Downloading C4 (alternative)\"):\n",
    "                text = example.get('text', '').strip()\n",
    "                if text:\n",
    "                    f.write(text + '\\n')\n",
    "                    lines_written += 1\n",
    "                    if lines_written >= max_lines:\n",
    "                        break\n",
    "        \n",
    "        print(f\"‚úì Downloaded C4 English: {lines_written:,} lines\")\n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Alternative failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download reasonable amount\n",
    "download_cc100_english('cc100_en_200k.txt', max_lines=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fed30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Cleaning cc100_en_200k.txt...\n",
      "  Processed 10 chunks...\n",
      "  Processed 20 chunks...\n",
      "  Processed 30 chunks...\n",
      "  Processed 40 chunks...\n",
      "  Processed 50 chunks...\n",
      "  Processed 60 chunks...\n",
      "  Processed 70 chunks...\n",
      "  Processed 80 chunks...\n",
      "  Processed 90 chunks...\n",
      "  Processed 100 chunks...\n",
      "  Processed 110 chunks...\n",
      "  Processed 120 chunks...\n",
      "  Processed 130 chunks...\n",
      "  Processed 140 chunks...\n",
      "  Processed 150 chunks...\n",
      "  Processed 160 chunks...\n",
      "  Processed 170 chunks...\n",
      "  Processed 180 chunks...\n",
      "  Processed 190 chunks...\n",
      "  Processed 200 chunks...\n",
      "  Processed 210 chunks...\n",
      "  Processed 220 chunks...\n",
      "  Processed 230 chunks...\n",
      "  Processed 240 chunks...\n",
      "  Processed 250 chunks...\n",
      "  Processed 260 chunks...\n",
      "  Processed 270 chunks...\n",
      "  Processed 280 chunks...\n",
      "  Processed 290 chunks...\n",
      "  Processed 300 chunks...\n",
      "  Processed 310 chunks...\n",
      "  Processed 320 chunks...\n",
      "  Processed 330 chunks...\n",
      "  Processed 340 chunks...\n",
      "  Processed 350 chunks...\n",
      "  Processed 360 chunks...\n",
      "  Processed 370 chunks...\n",
      "  Processed 380 chunks...\n",
      "  Processed 390 chunks...\n",
      "  Processed 400 chunks...\n",
      "  Processed 410 chunks...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 431,219,894 chars\n",
      "Cleaned: 287,856,338 chars\n",
      "Removed: 143,363,556 chars\n",
      "\n",
      "--- Sample ---\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat select\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_english_text_simple(text):\n",
    "    \"\"\"Simple English text cleaner similar to your Nepali cleaner.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Step 2: Keep only English characters, digits, and basic punctuation\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English letters\n",
    "        r'0-9'                       # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\-_\\'\"'            # Basic punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    # Step 4: Remove lines that are too short or too long\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if 20 <= len(line) <= 500:  # Reasonable line length\n",
    "            # Check if has at least 3 words\n",
    "            if len(line.split()) >= 3:\n",
    "                filtered_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "# Process file in chunks (identical to your Nepali processor)\n",
    "def clean_english_file_simple(input_file, output_file, chunk_size=1024*1024):\n",
    "    \"\"\"Simple English file cleaner with chunk processing.\"\"\"\n",
    "    \n",
    "    print(f\"üìñ Cleaning {input_file}...\")\n",
    "    \n",
    "    total_original = 0\n",
    "    total_cleaned = 0\n",
    "    chunk_num = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
    "             open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            \n",
    "            while True:\n",
    "                chunk = infile.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                chunk_num += 1\n",
    "                total_original += len(chunk)\n",
    "                \n",
    "                # Clean chunk\n",
    "                cleaned = clean_english_text_simple(chunk)\n",
    "                \n",
    "                # Write if not empty\n",
    "                if cleaned.strip():\n",
    "                    outfile.write(cleaned + '\\n')\n",
    "                    total_cleaned += len(cleaned)\n",
    "                \n",
    "                # Progress\n",
    "                if chunk_num % 10 == 0:\n",
    "                    print(f\"  Processed {chunk_num} chunks...\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: {input_file} not found\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample ---\")\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        sample = ''.join([f.readline() for _ in range(3)])\n",
    "        print(sample[:500])\n",
    "\n",
    "# Usage (just like your Nepali code)\n",
    "if __name__ == \"__main__\":\n",
    "    clean_english_file_simple(\n",
    "        input_file='cc100_en_200k.txt',\n",
    "        output_file='cc100_en_cleaned.txt',\n",
    "        chunk_size=1024*1024  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e19d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing news_wikipedia_ncc_corpus.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6707124it [00:29, 225239.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cc100_en_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1515908it [00:02, 536323.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 8,223,032\n",
      "Unique lines written: 8,222,518\n",
      "Output saved to: eng_news_wikipedia_ncc_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'news_wikipedia_ncc_corpus.txt'\n",
    "file2 = 'cc100_en_cleaned.txt'\n",
    "output_file = 'eng_news_wikipedia_ncc_corpus.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c802a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for eng_news_wikipedia_ncc_corpus (8.2M lines)...\n",
      "\n",
      "üìä Analyzing corpus composition...\n",
      "   File size: 3.64 GB\n",
      "   English-only lines: 0.0%\n",
      "   Nepali-only lines: 90.4%\n",
      "   Mixed lines: 9.6%\n",
      "   ‚Üí Optimizing for bilingual tokenization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8.2M corpus:   0%|          | 0/100 [00:00<?, ]"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Progress tracking using file size monitoring\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.running = True\n",
    "        self.pbar = tqdm(total=100, desc=\"Training\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {elapsed}')\n",
    "        \n",
    "    def monitor(self):\n",
    "        \"\"\"Monitor training by checking if model file exists and grows\"\"\"\n",
    "        last_size = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.running:\n",
    "            time.sleep(2)  # Check every 2 seconds\n",
    "            \n",
    "            if os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_size:\n",
    "                    # File is growing, training is progressing\n",
    "                    last_size = current_size\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.pbar.set_postfix({\"size\": f\"{current_size/1024:.1f}KB\", \n",
    "                                          \"time\": f\"{elapsed:.0f}s\"})\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "# Start monitoring\n",
    "monitor = TrainingMonitor('ne_spm')\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # Train with optimized settings\n",
    "    spm.SentencePieceTrainer.train(\n",
    "    input='eng_news_wikipedia_ncc_corpus.txt',\n",
    "    model_prefix='ne_spm_fixed',\n",
    "    vocab_size=64000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type='bpe',\n",
    "    \n",
    "    # ============ ADD THESE ============\n",
    "    split_by_whitespace=True,        # Respect word boundaries\n",
    "    split_by_unicode_script=True,    # Separate Devanagari/Latin scripts\n",
    "    split_by_number=True,            # Keep numbers separate\n",
    "    treat_whitespace_as_suffix=False, # Better word starts\n",
    "    byte_fallback=True,              # Handle rare chars gracefully\n",
    "    \n",
    "    # Optionally: add common English words as user symbols\n",
    "    user_defined_symbols=[\n",
    "        'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that',\n",
    "        'book', 'flight', 'customer', 'service', 'help', 'want', 'need'\n",
    "    ],\n",
    "    # ===================================\n",
    "    \n",
    "    input_sentence_size=6707124,\n",
    "    shuffle_input_sentence=True,\n",
    "    max_sentence_length=8192,\n",
    "    num_threads=16,\n",
    "    minloglevel=1\n",
    ")\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(f\"Model saved: ne_spm_fixed.model\")\n",
    "print(f\"Vocab saved: ne_spm_fixed.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0baed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 00:00"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Progress tracking using file size monitoring\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.running = True\n",
    "        self.pbar = tqdm(total=100, desc=\"Training\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {elapsed}')\n",
    "        \n",
    "    def monitor(self):\n",
    "        last_size = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.running:\n",
    "            time.sleep(2)\n",
    "            if os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_size:\n",
    "                    last_size = current_size\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.pbar.set_postfix(\n",
    "                        {\"size\": f\"{current_size/1024:.1f}KB\",\n",
    "                         \"time\": f\"{elapsed:.0f}s\"}\n",
    "                    )\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "# Start monitoring (FIXED PREFIX)\n",
    "monitor = TrainingMonitor('ne_spm_fixed')\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='eng_news_wikipedia_ncc_corpus.txt',\n",
    "        model_prefix='ne_spm_fixed',\n",
    "        vocab_size=64000,\n",
    "        character_coverage=0.9998,\n",
    "        model_type='bpe',\n",
    "\n",
    "        split_by_whitespace=True,\n",
    "        split_by_unicode_script=True,\n",
    "        split_by_number=True,\n",
    "        treat_whitespace_as_suffix=False,\n",
    "        byte_fallback=True,\n",
    "\n",
    "        user_defined_symbols=[\n",
    "            'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that',\n",
    "            'book', 'flight', 'customer', 'service', 'help', 'want', 'need'\n",
    "        ],\n",
    "\n",
    "        input_sentence_size=0,\n",
    "        shuffle_input_sentence=True,\n",
    "        max_sentence_length=8192,\n",
    "        num_threads=4,    # Kaggle-safe\n",
    "        minloglevel=0     # Show full logs\n",
    "    )\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(\"Model saved: ne_spm_fixed.model\")\n",
    "print(\"Vocab saved: ne_spm_fixed.vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf187e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
