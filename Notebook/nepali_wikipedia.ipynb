{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4476f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/disisbig/nepali-wikipedia-articles\n",
      "License(s): CC-BY-SA-4.0\n",
      "Downloading nepali-wikipedia-articles.zip to /home/lang-chain/Documents/daraz_product_review/Notebook\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 28.0M/28.4M [00:02<00:00, 11.9MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.4M/28.4M [00:02<00:00, 12.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download disisbig/nepali-wikipedia-articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ef9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Nepali Wikipedia articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38756/38756 [00:00<00:00, 41603.01it/s, Unique=22046, Dupes=3083, Missing=11408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Files processed: 38756\n",
      "Unique articles: 22440\n",
      "Duplicates removed: 3197\n",
      "Missing files: 11628\n",
      "Files with errors: 0\n",
      "\n",
      "Total characters: 17,474,756\n",
      "Average article length: 779 characters\n",
      "Output file size: 45.33 MB\n",
      "\n",
      "Output saved to: combined_nepali_wikipedia_progress.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm  # Install with: pip install tqdm\n",
    "\n",
    "def combine_with_progress_bar():\n",
    "    \"\"\"\n",
    "    Combine files with a progress bar for better visualization.\n",
    "    \"\"\"\n",
    "    base_dir = 'nepali-wikipedia-articles/train/train'\n",
    "    output_file = 'combined_nepali_wikipedia_progress.txt'\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Directory not found: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    seen_hashes = set()\n",
    "    stats = {\n",
    "        'unique': 0,\n",
    "        'duplicates': 0,\n",
    "        'missing': 0,\n",
    "        'errors': 0,\n",
    "        'total_chars': 0\n",
    "    }\n",
    "    \n",
    "    print(\"Combining Nepali Wikipedia articles...\")\n",
    "    \n",
    "    # Create a progress bar\n",
    "    with tqdm(total=38756, desc=\"Processing files\") as pbar:\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for i in range(38756):\n",
    "                filepath = os.path.join(base_dir, f\"{i}.txt\")\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "                            content = infile.read().strip()\n",
    "                            \n",
    "                            if content:\n",
    "                                content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "                                \n",
    "                                if content_hash not in seen_hashes:\n",
    "                                    seen_hashes.add(content_hash)\n",
    "                                    \n",
    "                                    # Add separator if not first article\n",
    "                                    if stats['unique'] > 0:\n",
    "                                        outfile.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "                                    \n",
    "                                    outfile.write(content)\n",
    "                                    stats['unique'] += 1\n",
    "                                    stats['total_chars'] += len(content)\n",
    "                                else:\n",
    "                                    stats['duplicates'] += 1\n",
    "                    except Exception as e:\n",
    "                        stats['errors'] += 1\n",
    "                        # Only show first few errors\n",
    "                        if stats['errors'] <= 3:\n",
    "                            tqdm.write(f\"Error reading {i}.txt: {str(e)[:50]}...\")\n",
    "                else:\n",
    "                    stats['missing'] += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Update progress bar description with current stats\n",
    "                if i % 1000 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'Unique': stats['unique'],\n",
    "                        'Dupes': stats['duplicates'],\n",
    "                        'Missing': stats['missing']\n",
    "                    })\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Files processed: 38756\")\n",
    "    print(f\"Unique articles: {stats['unique']}\")\n",
    "    print(f\"Duplicates removed: {stats['duplicates']}\")\n",
    "    print(f\"Missing files: {stats['missing']}\")\n",
    "    print(f\"Files with errors: {stats['errors']}\")\n",
    "    \n",
    "    if stats['unique'] > 0:\n",
    "        avg_length = stats['total_chars'] / stats['unique']\n",
    "        print(f\"\\nTotal characters: {stats['total_chars']:,}\")\n",
    "        print(f\"Average article length: {avg_length:.0f} characters\")\n",
    "        \n",
    "        # Calculate file size\n",
    "        if os.path.exists(output_file):\n",
    "            size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "            print(f\"Output file size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nOutput saved to: {output_file}\")\n",
    "\n",
    "# Run this directly\n",
    "if __name__ == \"__main__\":\n",
    "    combine_with_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00efd018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Processing combined_nepali_wikipedia.txt in chunks...\n",
      "  Processed 10 chunks (10,485,760 chars)...\n",
      "\n",
      "‚úì Done!\n",
      "Original: 19,314,754 chars\n",
      "Cleaned: 17,410,898 chars\n",
      "Removed: 1,903,856 chars\n",
      "\n",
      "--- Sample of cleaned text ---\n",
      "‡§Ö‡§®‡•ç‡§®‡§™‡•Ç‡§∞‡•ç‡§£ ‡§™‡•ã‡§∑‡•ç‡§ü ‡§è‡§ï ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§¶‡•à‡§®‡§ø‡§ï ‡§™‡§§‡•ç‡§∞‡§ø‡§ï‡§æ ‡§π‡•ã‡•§ ‡§Ø‡•ã ‡§™‡§§‡•ç‡§∞‡§ø‡§ï‡§æ‡§ï‡•ã ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç‡§Æ‡§æ ‡§õ ‡•§\n",
      "‡§ï‡§Ö‡§®‡•ç‡§®‡§™‡•Ç‡§∞‡•ç‡§£ ‡§™‡•ã‡§∑‡•ç‡§ü‡§ï‡•ã ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡§ü‡§ï ‡§™‡•ç‡§∞‡§ï‡§æ‡§∂‡§® 2058 ‡§∏‡§æ‡§≤ ‡§¨‡•à‡§∂‡§æ‡§ñ 18 ‡§ó‡§§‡•á ‡§≠‡§è‡§ï‡•ã ‡§π‡•ã‡•§\n",
      "================================================================================\n",
      "‡§™‡•ç‡§∞‡§•‡§Æ ‡§ï‡§æ‡§Å‡§ï‡§°‡§≠‡§ø‡§ü‡•ç‡§ü‡§æ ‡§ó‡•ã‡§≤‡•ç‡§°‡§ï‡§™‡§ï‡•ã ‡§â‡§™‡§æ‡§ß‡§ø ‡§≠‡•Å‡§ü‡§æ‡§®‡•Ä ‡§∂‡§∞‡§£‡§æ‡§∞‡•ç‡§•‡•Ä ‡§ï‡•ç‡§≤‡§¨‡§≤‡§æ‡§à ‡§´‡§æ‡§á‡§®‡§≤‡§Æ‡§æ 1-0 ‡§ï‡•ã ‡§ó‡•ã‡§≤‡§Ö‡§®‡•ç‡§§‡§∞‡§≤‡•á ‡§™‡§∞‡§æ‡§ú‡§ø‡§§ ‡§ó‡§∞‡•ç‡§¶‡•à ‡§µ‡§ø‡§∞‡•ç‡§§‡§æ‡§Æ‡•ã‡§° ‡§Ø‡•Å‡§®‡§æ‡§á‡§ü‡•á‡§° ‡§ï‡•ç‡§≤‡§≤‡•á‡§¨ ‡§â‡§™‡§æ‡§ß‡§ø‡§Æ‡§æ‡§•‡§ø ‡§ï‡§¨‡•ç‡§ú‡§æ ‡§ú‡§Æ‡§æ‡§è‡§ï‡•ã ‡§π‡•ã‡•§ ‡§ú‡•Ä‡§§‡§∏‡§Å‡§ó‡•à ‡§µ‡§ø‡§∞‡•ç‡§§‡§æ‡§Æ‡•ã‡§°‡§≤‡•á ‡§®‡§ó‡§¶ 1 ‡§≤‡§æ‡§ñ 1 ‡§π‡§ú‡§æ‡§∞ ‡§∞‡•Å‡§™‡•à‡§Ø‡§æ‡§Å‡§∏‡§π‡§ø‡§§ ‡§ó‡•ã‡§≤‡•ç‡§°‡§ï‡§™ ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ó‡§∞‡•á‡§ï‡•ã ‡§•‡§ø‡§Ø‡•ã‡•§ ‡§â‡§™‡§µ‡§ø‡§ú‡•á‡§§‡§æ ‡§∂‡§∞‡§£‡§æ‡§∞‡•ç‡§•‡•Ä ‡§ï‡•ç‡§≤‡§¨‡§≤‡•á ‡§®‡§ó\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_nepali_text(text):\n",
    "    \"\"\"Clean Nepali text while preserving Devanagari punctuation.\"\"\"\n",
    "    \n",
    "    # Step 1: Unicode normalization (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Keep Nepali-specific characters\n",
    "    allowed_pattern = re.compile(\n",
    "        r'[^'\n",
    "        r'a-zA-Z'                    # English\n",
    "        r'\\u0900-\\u097F'             # Devanagari including ‡•§‡••\n",
    "        r'0-9\\u0966-\\u096F'          # Digits\n",
    "        r'\\s'                        # Whitespace\n",
    "        r'.,!?;:()\\[\\]{}\\-\\'\\\"/\\\\'   # Punctuation\n",
    "        r']+'\n",
    "    )\n",
    "    text = allowed_pattern.sub(' ', text)\n",
    "    \n",
    "    # Step 3: Fix spacing\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process file in chunks to avoid memory crashes\n",
    "print(\"üìñ Processing combined_nepali_wikipedia.txt in chunks...\")\n",
    "\n",
    "chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "buffer = []\n",
    "total_original = 0\n",
    "total_cleaned = 0\n",
    "\n",
    "try:\n",
    "    with open('combined_nepali_wikipedia_progress.txt', 'r', encoding='utf-8') as infile, \\\n",
    "         open('combined_nepali_wikipedia_cleaned.txt', 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        chunk_num = 0\n",
    "        while True:\n",
    "            chunk = infile.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_num += 1\n",
    "            total_original += len(chunk)\n",
    "            \n",
    "            # Clean chunk\n",
    "            cleaned = clean_nepali_text(chunk)\n",
    "            \n",
    "            # Filter lines (minimum 5 chars)\n",
    "            lines = [line.strip() for line in cleaned.split('\\n') \n",
    "                     if len(line.strip()) >= 5]\n",
    "            \n",
    "            # Write to output\n",
    "            if lines:\n",
    "                output = '\\n'.join(lines) + '\\n'\n",
    "                outfile.write(output)\n",
    "                total_cleaned += len(output)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_original:,} chars)...\")\n",
    "    \n",
    "    print(f\"\\n‚úì Done!\")\n",
    "    print(f\"Original: {total_original:,} chars\")\n",
    "    print(f\"Cleaned: {total_cleaned:,} chars\")\n",
    "    print(f\"Removed: {total_original - total_cleaned:,} chars\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n--- Sample of cleaned text ---\")\n",
    "    with open('combined_nepali_wikipedia_progress.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read(500))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: ne.txt not found in current directory\")\n",
    "except MemoryError:\n",
    "    print(\"‚ùå Still running out of memory. Try increasing chunk_size or processing smaller sections.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcc0106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing combined_final.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6554129it [00:28, 230272.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing combined_nepali_wikipedia_cleaned.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66699it [00:00, 196899.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Combined + deduplicated successfully!\n",
      "Total lines seen: 6,620,828\n",
      "Unique lines written: 6,610,859\n",
      "Output saved to: wikipedia_ncc_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "file1 = 'combined_final.txt'\n",
    "file2 = 'combined_nepali_wikipedia_cleaned.txt'\n",
    "output_file = 'wikipedia_ncc_corpus.txt'\n",
    "\n",
    "def line_hash(text):\n",
    "    \"\"\"Fast MD5 hash for dedupe.\"\"\"\n",
    "    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "seen = set()\n",
    "total_written = 0\n",
    "total_seen = 0\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for file in [file1, file2]:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.strip()\n",
    "                total_seen += 1\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                h = line_hash(line)\n",
    "                if h in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(h)\n",
    "                out.write(line + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "print(\"\\n‚úì Combined + deduplicated successfully!\")\n",
    "print(f\"Total lines seen: {total_seen:,}\")\n",
    "print(f\"Unique lines written: {total_written:,}\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (6707124), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 06:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training complete!\n",
      "Model saved: ne_spm.model\n",
      "Vocab saved: ne_spm.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Progress tracking using file size monitoring\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.running = True\n",
    "        self.pbar = tqdm(total=100, desc=\"Training\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {elapsed}')\n",
    "        \n",
    "    def monitor(self):\n",
    "        \"\"\"Monitor training by checking if model file exists and grows\"\"\"\n",
    "        last_size = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.running:\n",
    "            time.sleep(2)  # Check every 2 seconds\n",
    "            \n",
    "            if os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_size:\n",
    "                    # File is growing, training is progressing\n",
    "                    last_size = current_size\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.pbar.set_postfix({\"size\": f\"{current_size/1024:.1f}KB\", \n",
    "                                          \"time\": f\"{elapsed:.0f}s\"})\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "# Start monitoring\n",
    "monitor = TrainingMonitor('ne_spm')\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # Train with optimized settings\n",
    "    spm.SentencePieceTrainer.train(\n",
    "    input='ne_cleaned.txt',\n",
    "    model_prefix='ne_spm_fixed',\n",
    "    vocab_size=32000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type='bpe',\n",
    "    \n",
    "    # ============ ADD THESE ============\n",
    "    split_by_whitespace=True,        # Respect word boundaries\n",
    "    split_by_unicode_script=True,    # Separate Devanagari/Latin scripts\n",
    "    split_by_number=True,            # Keep numbers separate\n",
    "    treat_whitespace_as_suffix=False, # Better word starts\n",
    "    byte_fallback=True,              # Handle rare chars gracefully\n",
    "    \n",
    "    # Optionally: add common English words as user symbols\n",
    "    user_defined_symbols=[\n",
    "        'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that',\n",
    "        'book', 'flight', 'customer', 'service', 'help', 'want', 'need'\n",
    "    ],\n",
    "    # ===================================\n",
    "    \n",
    "    input_sentence_size=6707124,\n",
    "    shuffle_input_sentence=True,\n",
    "    max_sentence_length=8192,\n",
    "    num_threads=16,\n",
    "    minloglevel=1\n",
    ")\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")\n",
    "print(f\"Model saved: ne_spm_fixed.model\")\n",
    "print(f\"Vocab saved: ne_spm_fixed.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29f38af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_final.txt                      ne_spm_fixed.model\n",
      "combined_nepali_wikipedia_cleaned.txt   ne_spm_fixed.vocab\n",
      "combined_nepali_wikipedia_progress.txt  ne.txt\n",
      "model.ipynb                             ne.txt.xz\n",
      "ne_cleaned.txt                          new_cleaned.txt\n",
      "ne_corpus_cleaned.txt                   news.ipynb\n",
      "ne_corpus.txt                           news.txt\n",
      "\u001b[0m\u001b[01;34mnepalinewsdataset\u001b[0m/                      news_wikipedia_ncc_corpus.txt\n",
      "nepalinewsdataset.zip                   \u001b[01;34moscar-corpus-nepali\u001b[0m/\n",
      "nepali_tokenizer_data.txt               oscar-corpus-nepali.zip\n",
      "\u001b[01;34mnepali-wikipedia-articles\u001b[0m/              practise.ipynb\n",
      "nepali-wikipedia-articles.zip           train.py\n",
      "nepali_wikipedia.ipynb                  train_tokenizer.py\n",
      "ne_spm_16V.model                        wikipedia_ncc_corpus.txt\n",
      "ne_spm_16V.vocab                        wikipedia_.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f35e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\n",
      "Tokens: ['‚ñÅ‡§®‡§Æ‡§∏‡•ç‡§§‡•á', ',', '‚ñÅ‡§Æ‡•á‡§∞‡•ã', '‚ñÅ‡§®‡§æ‡§Æ', '‚ñÅ‡§∞‡§æ‡§ú', '‚ñÅ‡§π‡•ã', '‡•§']\n",
      "Token IDs: [13137, 31911, 1050, 976, 474, 379, 31898]\n",
      "Decoded: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\n",
      "\n",
      "Text: I want to book a flight to Kathmandu.\n",
      "Tokens: ['‚ñÅI', '‚ñÅ', 'want', '‚ñÅ', 'to', '‚ñÅ', 'book', '‚ñÅ', 'a', '‚ñÅ', 'flight', '‚ñÅ', 'to', '‚ñÅ', '<0x4B>', 'a', 'thm', 'and', 'u', '.']\n",
      "Token IDs: [2835, 31873, 18, 31873, 5, 31873, 13, 31873, 7, 31873, 14, 31873, 5, 31873, 95, 7, 15254, 4, 31965, 31920]\n",
      "Decoded: I want to book a flight to Kathmandu.\n",
      "\n",
      "Text: ‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\n",
      "Tokens: ['‚ñÅ‡§Æ', '‚ñÅ‡§Ü‡§ú', '‚ñÅ', 'of', 'f', 'ice', '‚ñÅ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å', '‡•§']\n",
      "Token IDs: [299, 630, 31873, 6, 31998, 16799, 13980, 31898]\n",
      "Decoded: ‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\n",
      "\n",
      "Text: Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\n",
      "Tokens: ['‚ñÅC', 'us', 'to', 'm', 'er', '‚ñÅ', 'service', '‚ñÅ‡§≤‡•á', '‚ñÅ', 'help', '‚ñÅ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ', '‡•§']\n",
      "Token IDs: [1643, 5854, 5, 31952, 1587, 31873, 16, 626, 31873, 17, 2397, 31898]\n",
      "Decoded: Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('ne_spm_fixed.model')\n",
    "\n",
    "# Test on mixed Nepali-English samples\n",
    "test_samples = [\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§∞‡§æ‡§ú ‡§π‡•ã‡•§\",  # Pure Nepali\n",
    "    \"I want to book a flight to Kathmandu.\",  # Pure English\n",
    "    \"‡§Æ ‡§Ü‡§ú office ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å‡•§\",  # Code-mixed\n",
    "    \"Customer service ‡§≤‡•á help ‡§ó‡§∞‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§\"  # Mixed domain\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {sp.encode(text)}\")\n",
    "    print(f\"Decoded: {sp.decode(sp.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a27e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OPTIMIZED training for Nepali-dominant corpus (8.2M lines)...\n",
      "\n",
      "üìä Analyzing corpus composition...\n",
      "   English-only lines: 0.0%\n",
      "   Nepali-only lines: 90.4%\n",
      "   Mixed lines: 9.6%\n",
      "   ‚Üí Prioritizing Nepali tokenization with English support...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/100 [00:00<?]"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Starting OPTIMIZED training for Nepali-dominant corpus (8.2M lines)...\")\n",
    "\n",
    "# ============ CORPUS ANALYSIS ============\n",
    "print(\"\\nüìä Analyzing corpus composition...\")\n",
    "try:\n",
    "    import re\n",
    "    \n",
    "    with open('eng_news_wikipedia_ncc_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "        sample_lines = [next(f) for _ in range(min(10000, 8222518))]\n",
    "    \n",
    "    devanagari_pattern = re.compile(r'[\\u0900-\\u097F]')\n",
    "    latin_pattern = re.compile(r'[a-zA-Z]')\n",
    "    \n",
    "    eng_only = sum(1 for line in sample_lines if latin_pattern.search(line) and not devanagari_pattern.search(line))\n",
    "    nep_only = sum(1 for line in sample_lines if devanagari_pattern.search(line) and not latin_pattern.search(line))\n",
    "    mixed = sum(1 for line in sample_lines if devanagari_pattern.search(line) and latin_pattern.search(line))\n",
    "    \n",
    "    print(f\"   English-only lines: {eng_only/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   Nepali-only lines: {nep_only/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   Mixed lines: {mixed/len(sample_lines)*100:.1f}%\")\n",
    "    print(f\"   ‚Üí Prioritizing Nepali tokenization with English support...\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not analyze corpus: {e}\\n\")\n",
    "\n",
    "# Enhanced progress tracking\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = f\"{model_prefix}.model\"\n",
    "        self.running = True\n",
    "        self.pbar = tqdm(total=100, desc=\"Training progress\", unit=\"%\", \n",
    "                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "        \n",
    "    def monitor(self):\n",
    "        last_size = 0\n",
    "        start_time = time.time()\n",
    "        estimated_final_size = 2048 * 1024  # ~2MB\n",
    "        \n",
    "        while self.running:\n",
    "            time.sleep(3)\n",
    "            \n",
    "            if os.path.exists(self.model_file):\n",
    "                current_size = os.path.getsize(self.model_file)\n",
    "                if current_size > last_size:\n",
    "                    last_size = current_size\n",
    "                    elapsed = time.time() - start_time\n",
    "                    progress = min(95, (current_size / estimated_final_size) * 100)\n",
    "                    self.pbar.n = int(progress)\n",
    "                    self.pbar.set_postfix({\n",
    "                        \"size\": f\"{current_size/1024:.1f}KB\", \n",
    "                        \"time\": f\"{elapsed:.0f}s\"\n",
    "                    })\n",
    "                    self.pbar.refresh()\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.pbar.n = 100\n",
    "        self.pbar.set_postfix({\"status\": \"Complete!\"})\n",
    "        self.pbar.refresh()\n",
    "        self.pbar.close()\n",
    "\n",
    "monitor = TrainingMonitor('nepali_optimized_spm')\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    # OPTIMIZED for 90% Nepali, 10% English/Mixed\n",
    "    training_args = {\n",
    "        # ============ CORPUS SETTINGS ============\n",
    "        'input': 'eng_news_wikipedia_ncc_corpus.txt',\n",
    "        'model_prefix': 'nepali_optimized_spm',\n",
    "        'vocab_size': 50000,  # Reduced - focused on Nepali\n",
    "        'character_coverage': 0.9999,  # Very high for Devanagari priority\n",
    "        'model_type': 'bpe',\n",
    "        \n",
    "        # ============ MEMORY & PERFORMANCE ============\n",
    "        'train_extremely_large_corpus': True,\n",
    "        'input_sentence_size': 5000000,  # Reduced to 5M for stability\n",
    "        'shuffle_input_sentence': True,\n",
    "        'max_sentence_length': 8192,  # Increased to handle long lines\n",
    "        'max_sentencepiece_length': 20,  # Increased for better Devanagari tokens\n",
    "        'num_threads': 6,  # Conservative threading\n",
    "        'minloglevel': 1,\n",
    "        \n",
    "        # ============ NEPALI-OPTIMIZED MULTILINGUAL SETTINGS ============\n",
    "        'split_by_whitespace': True,\n",
    "        'split_by_unicode_script': False,  # Allow mixed script tokens\n",
    "        'split_by_number': False,  # Keep Devanagari numerals intact\n",
    "        'treat_whitespace_as_suffix': False,\n",
    "        'byte_fallback': True,\n",
    "        'normalization_rule_name': 'nfkc',  # Better for Devanagari\n",
    "        'add_dummy_prefix': True,\n",
    "        'remove_extra_whitespaces': True,\n",
    "        \n",
    "        # ============ VOCABULARY (Nepali-focused) ============\n",
    "        'user_defined_symbols': [\n",
    "            # Critical Nepali postpositions & particles\n",
    "            '‡§ï‡•ã', '‡§õ', '‡§∞', '‡§Æ‡§æ', '‡§≤‡•á', '‡§≤‡§æ‡§à', '‡§¨‡§æ‡§ü', '‡§ï‡§æ', '‡§ï‡•Ä', '‡§™‡§®‡§ø',\n",
    "            '‡§§', '‡§®‡•à', '‡§∏‡§Æ‡•ç‡§Æ', '‡§¶‡•á‡§ñ‡§ø', '‡§≠‡§®‡•ç‡§®‡•á', '‡§ó‡§∞‡•ç‡§®‡•á', '‡§ó‡§∞‡•ç‡§®', '‡§π‡•Å‡§®', '‡§≠‡§è‡§ï‡•ã',\n",
    "            '‡§∞‡§π‡•á‡§ï‡•ã', '‡§Ü‡§è‡§ï‡•ã', '‡§ó‡§è‡§ï‡•ã', '‡§≠‡§è', '‡§õ‡§®‡•ç', '‡§•‡§ø‡§Ø‡•ã', '‡§•‡§ø‡§è', '‡§π‡•Å‡§®‡•ç‡§õ',\n",
    "            \n",
    "            # High-frequency Nepali nouns/verbs\n",
    "            '‡§®‡•á‡§™‡§æ‡§≤', '‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç', '‡§∏‡§∞‡§ï‡§æ‡§∞', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞', '‡§ú‡§®‡§§‡§æ', '‡§Æ‡§æ‡§®‡§ø‡§∏', '‡§¶‡•á‡§∂',\n",
    "            '‡§µ‡§ø‡§∂‡•ç‡§µ', '‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø', '‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø', '‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä', '‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä',\n",
    "            '‡§∏‡§Ç‡§∏‡§¶', '‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®', '‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä', '‡§∏‡§Æ‡§Ø', '‡§¶‡§ø‡§®', '‡§µ‡§∞‡•ç‡§∑', '‡§Æ‡§π‡§ø‡§®‡§æ',\n",
    "            '‡§µ‡§ø‡§ï‡§æ‡§∏', '‡§Ø‡•ã‡§ú‡§®‡§æ', '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ', '‡§®‡•Ä‡§§‡§ø', '‡§ï‡§æ‡§®‡•Ç‡§®', '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞',\n",
    "            \n",
    "            # Nepali conjunctions & connectors\n",
    "            '‡§§‡§∞', '‡§Ø‡§¶‡§ø', '‡§Ø‡§∏‡§∞‡•Ä', '‡§ï‡§ø‡§®‡§≠‡§®‡•á', '‡§Ø‡§¶‡•ç‡§Ø‡§™‡§ø', '‡§§‡§•‡§æ‡§™‡§ø', '‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç',\n",
    "            \n",
    "            # Essential English (for 10% mixed content)\n",
    "            'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'that', 'with',\n",
    "            'said', 'news', 'government', 'according', 'report', 'wikipedia',\n",
    "            \n",
    "            # Special tokens\n",
    "            '<NEP>', '<ENG>', '<NUM>', '<DATE>', '<URL>',\n",
    "        ],\n",
    "        \n",
    "        # ============ STANDARD SETTINGS ============\n",
    "        'unk_id': 0,\n",
    "        'bos_id': 1,\n",
    "        'eos_id': 2,\n",
    "        'pad_id': -1,\n",
    "        'unk_piece': '<unk>',\n",
    "        'bos_piece': '<s>',\n",
    "        'eos_piece': '</s>',\n",
    "        'pad_piece': '<pad>',\n",
    "        'hard_vocab_limit': True,\n",
    "    }\n",
    "    \n",
    "    arg_string = ' '.join([f'--{k}={v}' if not isinstance(v, list) else \n",
    "                           ' '.join([f'--{k}={item}' for item in v]) \n",
    "                           for k, v in training_args.items()])\n",
    "    \n",
    "    print(f\"üîß Training with Nepali-optimized parameters...\")\n",
    "    spm.SentencePieceTrainer.train(arg_string)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"1. Verify input file: eng_news_wikipedia_ncc_corpus.txt\")\n",
    "    print(\"2. Check available RAM (recommend 8GB+)\")\n",
    "    print(\"3. Reduce input_sentence_size to 3000000\")\n",
    "    print(\"4. Reduce vocab_size to 32000\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    monitor.stop()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Corpus: 8,222,518 lines (90% Nepali, 10% Mixed/English)\")\n",
    "print(f\"ü§ñ Model: nepali_optimized_spm.model\")\n",
    "print(f\"üìö Vocab: nepali_optimized_spm.vocab\")\n",
    "print(f\"üéØ Vocab size: 50,000 tokens (Nepali-optimized)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============ COMPREHENSIVE TESTING ============\n",
    "print(\"\\nüß™ Testing tokenizer performance...\")\n",
    "\n",
    "try:\n",
    "    sp = spm.SentencePieceProcessor(model_file='nepali_optimized_spm.model')\n",
    "    \n",
    "    test_cases = [\n",
    "        # Pure Nepali\n",
    "        (\"‡§Ü‡§ú‡§ï‡•ã ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞: ‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§®‡§Ø‡§æ‡§Å ‡§®‡•Ä‡§§‡§ø ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ó‡§∞‡•ç‡§Ø‡•ã‡•§\", \"Pure Nepali\"),\n",
    "        (\"‡§µ‡§ø‡§ï‡§ø‡§™‡§ø‡§°‡§ø‡§Ø‡§æ ‡§è‡§ï ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§Ö‡§®‡§≤‡§æ‡§á‡§® ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡•ã‡§∂ ‡§π‡•ã‡•§\", \"Pure Nepali\"),\n",
    "        (\"‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç‡§Æ‡§æ ‡§Ü‡§ú ‡•®‡•´ ‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä ‡§§‡§æ‡§™‡§ï‡•ç‡§∞‡§Æ ‡§õ‡•§\", \"Nepali with numbers\"),\n",
    "        \n",
    "        # Mixed content (your 10%)\n",
    "        (\"Nepal ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á Wikipedia ‡§Æ‡§æ article ‡§≤‡•á‡§ñ‡•ç‡§Ø‡•ã‡•§\", \"Code-switched\"),\n",
    "        (\"‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä‡§≤‡•á BBC interview ‡§¶‡§ø‡§è‡•§\", \"Code-switched\"),\n",
    "        \n",
    "        # Pure English (for comparison)\n",
    "        (\"Breaking news: Nepal's government announced new policies.\", \"Pure English\"),\n",
    "        (\"Wikipedia is a free online encyclopedia.\", \"Pure English\"),\n",
    "        \n",
    "        # Complex Nepali\n",
    "        (\"‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§µ‡§ø‡§ï‡§æ‡§∏‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§®‡§Ø‡§æ‡§Å ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§§‡§∞‡•ç‡§ú‡•Å‡§Æ‡§æ ‡§ó‡§∞‡•á‡§ï‡•ã ‡§õ‡•§\", \"Complex Nepali\"),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    for sentence, label in test_cases:\n",
    "        tokens = sp.encode(sentence, out_type=str)\n",
    "        token_count = len(tokens)\n",
    "        efficiency = len(sentence) / token_count\n",
    "        \n",
    "        print(f\"\\n[{label}]\")\n",
    "        print(f\"Text: '{sentence[:60]}{'...' if len(sentence) > 60 else ''}'\")\n",
    "        print(f\"Tokens ({token_count}): {tokens[:8]}{'...' if token_count > 8 else ''}\")\n",
    "        print(f\"Efficiency: {efficiency:.1f} chars/token\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà Final vocabulary size: {sp.vocab_size():,}\")\n",
    "    print(f\"üî§ First 15 tokens: {[sp.id_to_piece(i) for i in range(15)]}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Vocabulary composition analysis\n",
    "    vocab_sample = [sp.id_to_piece(i) for i in range(100, 200)]\n",
    "    devanagari_count = sum(1 for token in vocab_sample if any('\\u0900' <= c <= '\\u097F' for c in token))\n",
    "    print(f\"\\nüìä Vocab composition (sample 100-200):\")\n",
    "    print(f\"   Devanagari tokens: {devanagari_count}/100 ({devanagari_count}%)\")\n",
    "    print(f\"   ‚Üí Good balance for your 90% Nepali corpus\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Testing failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Tokenizer ready for deployment!\")\n",
    "print(\"üí° Next steps:\")\n",
    "print(\"   1. Compare token counts between old and new model\")\n",
    "print(\"   2. Test on your actual downstream task\")\n",
    "print(\"   3. Monitor OOV rate on validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0ac8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
